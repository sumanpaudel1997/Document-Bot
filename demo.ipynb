{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "import os\n",
    "\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\", temperature=0.2, max_tokens=1024, anthropic_api_key=anthropic_api_key)\n",
    "\n",
    "# claude-3-sonnet-20240229\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can assist with testing in several ways:\\n\\n1. **Test Case Generation**: Langsmith can be used to generate test cases automatically based on the requirements or specifications of the software being tested. This can help in achieving better test coverage and identifying edge cases that might have been missed during manual test case creation.\\n\\n2. **Natural Language Test Case Creation**: Langsmith can help in converting natural language requirements or user stories into executable test cases. This can be particularly useful in agile development environments where requirements are often expressed in natural language.\\n\\n3. **Test Data Generation**: Langsmith can be used to generate realistic test data, including edge cases and boundary conditions, based on the data requirements of the application under test.\\n\\n4. **Test Automation**: Langsmith can assist in generating code for automated tests, such as unit tests or integration tests, based on the test cases or requirements provided.\\n\\n5. **Test Reporting**: Langsmith can help in generating human-readable test reports by summarizing the test results and providing insights into the testing process.\\n\\n6. **Test Maintenance**: As requirements or code changes, Langsmith can assist in updating the existing test cases or generating new test cases to ensure that the test suite remains up-to-date and relevant.\\n\\n7. **Test Refactoring**: Langsmith can help in refactoring and optimizing existing test cases, making them more readable, maintainable, and efficient.\\n\\n8. **Test Documentation**: Langsmith can be used to generate documentation for test cases, test plans, and test strategies, ensuring that the testing process is well-documented and easy to understand for all stakeholders.\\n\\nIt's important to note that while Langsmith can be a powerful tool for testing, it should be used in conjunction with human expertise and domain knowledge to ensure the quality and accuracy of the generated artifacts.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm | prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I was created by Anthropic, an artificial intelligence research company. They used large language models and machine learning techniques to train me, but the details of my training process are not something I'm fully aware of. I'm an artificial intelligence, not a human, and the specifics of my inner workings and creation are not something I have complete insight into. I'm happy to discuss my capabilities and what I can do, but the technical details of my development are not something I can speak to with full certainty.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how were you created?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader('spark.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1\\nSpark\\n \\nThe Definitive Guide\\nExcerpts from the upcoming book on making  \\nbig data simple with Apache Spark.\\nBy Bill Chambers & Matei Zaharia\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 0, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Apache Spark has seen immense growth over the past \\nseveral years. The size and scale of Spark Summit 2017 is \\na true reflection of innovation after innovation that has \\nmade itself into the Apache Spark project. Databricks \\nis proud to share excerpts from the upcoming book, \\nSpark: The Definitive Guide. Enjoy this free preview copy, \\ncourtesy of Databricks, of chapters 2, 3, 4, and 5 and \\nsubscribe to the Databricks blog for upcoming chapter \\nreleases.\\nPreface\\n2\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 1, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='A Gentle Introduction to Spark\\nThis chapter will present a gentle introduction to Spark. We will walk through the core architecture of a cluster, Spark \\nApplication, and Spark’s Structured APIs using DataFrames and SQL. Along the way we will touch on Spark’s core \\nterminology and concepts so that you are empowered start using Spark right away. Let’s get started with some basic \\nbackground terminology and concepts.\\nSpark’s Basic Architecture\\nTypically when you think of a “computer” you think about one machine sitting on your desk at home or at work. This \\nmachine works perfectly well for watching movies or working with spreadsheet software. However, as many users \\nlikely experience at some point, there are some things that your computer is not powerful enough to perform. One \\nparticularly challenging area is data processing. Single machines do not have enough power and resources to perform \\ncomputations on huge amounts of information (or the user may not have time to wait for the computation to finish). \\nA cluster, or group of machines, pools the resources of many machines together allowing us to use all the cumulative \\nresources as if they were one. Now a group of machines alone is not powerful, you need a framework to coordinate \\nwork across them. Spark is a tool for just that, managing and coordinating the execution of tasks on data across a \\ncluster of computers.\\nThe cluster of machines that Spark will leverage to execute tasks will be managed by a cluster manager like Spark’s \\nStandalone cluster manager, YARN, or Mesos. We then submit Spark Applications to these cluster managers which will \\ngrant resources to our application so that we can complete our work.\\nSpark Applications\\nSpark Applications consist of a driver process and a set of executor processes. The driver process, Figure 1-2, sits \\non a node in the cluster and is responsible for three things: maintaining information about the Spark application; \\nresponding to a user’s program; and analyzing, distributing, and scheduling work across the executors. As suggested \\nby the following figure, the driver process is absolutely essential - it’s the heart of a Spark Application and maintains \\nall relevant information during the lifetime of the application.\\n3\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 2, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='The driver maintains the work to be done, the executors are responsible for only two things: executing code assigned \\nto it by the driver and reporting the state of the computation, on that executor, back to the driver node.\\nThe last piece relevant piece for us is the cluster manager. The cluster manager controls physical machines and \\nallocates resources to Spark applications. This can be one of several core cluster managers: Spark’s standalone \\ncluster manager, YARN, or Mesos. This means that there can be multiple Spark applications running on a cluster at \\nthe same time. We will talk more in depth about cluster managers in Part IV: Production Applications of this book. In \\nthe previous illustration we see on the left, our driver and on the right the four executors on the right. In this diagram, \\nwe removed the concept of cluster nodes. The user can specify how many executors should fall on each node through \\nconfigurations.\\nnote\\nSpark, in addition to its cluster mode, also has a local mode. The driver and executors are simply processes, this \\nmeans that they can live on a single machine or multiple machines. In local mode, these run (as threads) on your \\nindividual computer instead of a cluster. We wrote this book with local mode in mind, so everything should be \\nrunnable on a single machine.\\nAs a short review of Spark Applications, the key points to understand at this point are that:\\n• Spark has some cluster manager that maintains an understanding of the resources available.\\n• The driver process is responsible for executing our driver program’s commands accross the executors in order to \\ncomplete our task.\\nNow while our executors, for the most part, will always be running Spark code. Our driver can be “driven” from a \\nnumber of different languages through Spark’s Language APIs.\\nFigure \\nJVM\\nUser Code\\nTo Executors\\nSpark Session\\nSpark Application\\n4\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 3, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Figure 2:\\nDriver Process\\nExecutors\\nUser Code\\nSpark Session\\nCluster Manager\\nSpark’s Language APIs\\nSpark’s language APIs allow you to run Spark code from other langauges. For the most part, Spark presents some core \\n“concepts” in every language and these concepts are translated into Spark code that runs on the cluster of machines. \\nIf you use the Structured APIs (Part II of this book), you can expect all languages to have the same performance \\ncharacteristics.\\nnote\\nThis is a bit more nuanced than we are letting on at this point but for now, it’s true “enough”. We cover this \\nextensively in first chapters of Part II of this book.\\n5\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 4, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Scala\\nSpark is primarily written in Scala, making it Spark’s “default” language. This book will include Scala code examples \\nwherever relevant.\\nPython\\nPython supports nearly all constructs that Scala supports. This book will include Python code examples whenever we \\ninclude Scala code examples and a Python API exists.\\nSQL\\nSpark supports ANSI SQL 2003 standard. This makes it easy for analysts and non-programmers to leverage the big \\ndata powers of Spark. This book will include SQL code examples wherever relevant\\nJava\\nEven though Spark is written in Scala, Spark’s authors have been careful to ensure that you can write Spark code in \\nJava. This book will focus primarily on Scala but will provide Java examples where relevant.\\nR\\nSpark has two libraries, one as a part of Spark core (SparkR) and another as a R community driven package (sparklyr). \\nWe will cover these two different integrations in Part VII: Ecosystem. \\n6\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 5, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Here’s a simple illustration of this relationship.\\nEach language API will maintain the same core concepts that we described above. There is a SparkSession available \\nto the user, the SparkSession will be the entrance point to running Spark code. When using Spark from a Python or \\nR, the user never writes explicit JVM instructions, but instead writes Python and R code that Spark will translate into \\ncode that Spark can then run on the executor JVMs. There is plenty more detail about this implementation that we \\ncover in later parts of the book but for the most part the above section should be plenty for you to use and leverage \\nSpark successfully.\\nStarting Spark\\nThus far we covered the basics concepts of Spark Applications. At this point it’s time to dive into Spark itself and \\nunderstand how we actually go about leveraging Spark. To do this we will start Spark’s local mode, just like we did \\nin the previous chapter, this means running ./bin/spark-shell to access the Scala console. You can also start \\nPython console with ./bin/pyspark. This starts an interactive Spark Application. There is another method for \\nsubmitting applications to Spark called spark-submit which does not allow for a user console but instead executes \\nprepared user code on the cluster as its own application. We discuss spark-submit in Part IV of the book. When we \\nstart Spark in this interactive mode, we implicitly create a SparkSession which manages the Spark Application.\\nSparkSession\\nAs discussed in the beginning of this chapter, we control our Spark Application through a driver process. This driver \\nprocess manifests itself to the user as something called the SparkSession. The SparkSession instance is the way Spark \\nexeutes user-defined manipulations across the cluster. In Scala and Python the variable is available as spark when \\nyou start up the console. Let’s go ahead and look at the SparkSession in both Scala and/or Python.\\nFigure \\nSpark Session\\nUser Code\\nJVM\\nPython/R Process\\n7\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 6, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='spark\\nIn Scala, you should see something like:\\nres0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@27159a24\\nIn Python you’ll see something like:\\n<pyspark.sql.session.SparkSession at 0x7efda4c1ccd0>\\nLet’s now perform the simple task of creating a range of numbers. This range of numbers is just like a named column  \\nin a spreadsheet.\\n%scala\\nval myRange = spark.range(1000).toDF(“number”)\\n%python\\nmyRange = spark.range(1000).toDF(“number”)\\nYou just ran your first Spark code! We created a DataFrame with one column containing 1000 rows with values from \\n0 to 999. This range of number represents a distributed collection. When run on a cluster, each part of this range of \\nnumbers exists on a different executor. This range is what Spark defines as a DataFrame.\\nDataFrames\\nA DataFrame is a table of data with rows and columns. The list of columns and the types in those columns is the \\nschema. A simple analogy would be a spreadsheet with named columns. The fundamental difference is that while a \\nspreadsheet sits on one computer in one specific location, a Spark DataFrame can span thousands of computers. The \\nreason for putting the data on more than one computer should be intuitive: either the data is too large to fit on one \\nmachine or it would simply take too long to perform that computation on one machine. The DataFrame concept is not \\nunique to Spark. R and Python both have similar concepts. However, Python/R DataFrames (with some exceptions) \\nexist on one machine rather than multiple machines. This limits what you can do with a given DataFrame in python \\nand R to the resources that exist on that specific machine. However, since Spark has language interfaces for both \\nPython and R, it’s quite easy to convert to Pandas (Python) DataFrames\\n8\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 7, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='note\\nSpark has several core abstractions: Datasets, DataFrames, SQL Tables, and Resilient Distributed Datasets \\n(RDDs). These abstractions all represent distributed collections of data however they have different interfaces \\nfor working with that data. The easiest and most efficient are DataFrames, which are available in all languages. \\nWe cover Datasets at the end of Part II and RDDs in Part III of this book. The following concepts apply to all of the \\ncore abstractions.\\nPartitions\\nIn order to allow every executor to perform work in parallel, Spark breaks up the data into chunks, called partitions. A \\npartition is a collection of rows that sit on one physical machine in our cluster. A DataFrame’s partitions represent how \\nthe data is physically distributed across your cluster of machines during execution. If you have one partition, Spark \\nwill only have a parallelism of one even if you have thousands of executors. If you have many partitions, but only one \\nexecutor Spark will still only have a parallelism of one because there is only one computation resource. \\nAn important thing to note, is that with DataFrames, we do not (for the most part) manipulate partitions individually. \\nWe simply specify high level transformations of data in the physical partitions and Spark determines how this work \\nwill actually execute on the cluster. Lower level APIs do exist (via the RDD interface) and we cover those in Part III of \\nthis book.\\nTransformations\\nIn Spark, the core data structures are immutable meaning they cannot be changed once created. This might seem like \\na strange concept at first, if you cannot change it, how are you supposed to use it? In order to “change” a DataFrame \\nyou will have to instruct Spark how you would like to modify the DataFrame you have into the one that you want. \\nThese instructions are called transformations. \\nFigure \\nTable or DataFrame partitioned \\nacross servers in data center\\nSpreadsheet on a \\nsingle machine\\n9\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 8, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Let’s perform a simple transformation to find all even numbers in our currentDataFrame.\\n%scala\\nval divisBy2 = myRange.where(“number % 2 = 0”)\\n%python\\ndivisBy2 = myRange.where(“number % 2 = 0”)\\nYou will notice that these return no output, that’s because we only specified an abstract transformation and Spark \\nwill not act on transformations until we call an action, discussed shortly. Transformations are the core of how you \\nwill be expressing your business logic using Spark. There are two types of transformations, those that specify narrow \\ndependencies and those that specify wide dependencies. Transformations consisting of narrow dependencies \\nare those where each input partition will contribute to only one output partition. Our where clause specifies a \\nnarrow dependency, where only one partition contributes to at most one output partition. A wide dependency style \\ntransformation will have input partitions contributing to many output partitions. We call this a shuffle where Spark \\nwill exchange partitions across the cluster. Spark will automatically perform an operation called pipelining on narrow \\ndependencies, this means that if we specify multiple filters on DataFrames they’ll all be performed in memory. The \\nsame cannot be said for shuffles. When we perform a shuffle, Spark will write the results to disk. You’ll see lots of talks \\nabout shuffle optimization across the web because it’s an important topic but for now all you need to understand are \\nthat there are two kinds of transformations.\\nThis brings ups our next concept, transformations are abstract manipulations of data that Spark will execute lazily.\\nLazy Evaluation\\nLazy evaulation means that Spark will wait until the very last moment to execute your transformations. In Spark, \\ninstead of modifying the data quickly, we build up a plan of transformations that we would like to apply to our source \\ndata. Spark, by waiting until the last minute to execute the code, will compile this plan from your raw, DataFrame \\ntransformations, to an efficient physical plan that will run as efficiently as possible across the cluster. This provides \\nimmense benefits to the end user because Spark can optimize the entire data flow from end to end. An example of this \\nmight be “predicate pushdown”. If we build a large Spark job consisting of narrow dependencies, but specify a filter at \\nthe end that only requires us to fetch one row from our source data, the most efficient way to execute this is to access \\nthe single record that we need. Spark will actually optimize this for us by pushing the filter down automatically.\\n10\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 9, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Actions\\nTransformations allow us to build up our logical transformation plan. To trigger the computation, we run an action. An \\naction instructs Spark to compute a result from a series of transformations. The simplest action is count which gives \\nus the total number of records in the DataFrame.\\ndivisBy2.count()\\nWe now see a result! There are 500 number divisible by two from o to 999 (big surprise!). Now count is not the only \\naction. There are three kinds of actions:\\n• actions to view data in the console;\\n• actions to collect data to native objects in the respective language;\\n• and actions to write to output data sources.\\nIn specifying our action, we started a Spark job that runs our filter transformation (a narrow transformation), then an \\naggregation (a wide transformation) that performs the counts on a per partition basis, then a collect will brings our \\nresult to a native object in the respective language. We can see all of this by inspecting the Spark UI, a tool included in \\nSpark that allows us to monitor the Spark jobs running on a cluster.\\nSpark UI\\nDuring Spark’s execution of the previous code block, users can monitor the progress of their job through the Spark UI. \\nThe Spark UI is available on port 4040 of the driver node. If you are running in local mode this will just be the  \\nhttp://localhost:4040. The Spark UI maintains information on the state of our Spark jobs, environment, and \\ncluster state. It’s very useful, especially for tuning and debugging. In this case, we can see one Spark job with two \\nstages and nine tasks were executed. \\nThis chapter avoids the details of Spark jobs and the Spark UI. At this point you should understand that a Spark job \\nrepresents a set of transformations triggered by an individual action and we can monitor that from the Spark UI. We \\ndo cover the Spark UI in detail in Part IV: Production Applications of this book.\\n11\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 10, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Figure 5\\nAn End to End Example\\nIn the previous example, we created a DataFrame of a range of numbers. Not exactly groundbreaking big data. In \\nthis section we will reinforce everything we learned previously in this chapter with a worked example and explaining \\nstep by step what is happening under the hood. We’ll be using some flight data available here from the United States \\nBureau of Transportation statistics.\\nInside of the CSV folder linked above, you’ll see that we have a number of files. You will also notice a number of other \\nfolders with different file formats that we will discuss in Part II: Reading and Writing data.\\n%fs ls /mnt/defg/flight-data/csv/\\nEach file has a number of rows inside of it. Now these files are CSV files, meaning that they’re a semi-structured data \\nformat with a row in the file representing a row in our future DataFrame.\\n$ head /mnt/defg/flight-data/csv/2015-summary.csv\\nDEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count\\nUnited States,Romania,15\\nUnited States,Croatia,1\\nUnited States,Ireland,344\\n12\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 11, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Spark includes the ability to read and write from a large number of data sources. In order to read this data in, we will \\nuse a DataFrameReader that is associated with our SparkSession. In doing so, we will specify the file format as well as \\nany options we want to specify. In our case, we want to do something called schema inference, we want Spark to take \\na best guess at what the schema of our DataFrame should be. The reason for this is that CSV files are not completely \\nstructured data formats. We also want to specify that the first row is the header in the file, we’ll specify that as an \\noption too.\\nTo get this information Spark will read in a little bit of the data and then attempt to parse the types in those rows \\naccording to the types available in Spark. You’ll see that it does a good job. We also have the option of strictly \\nspecifying a schema when we read in data.\\n%scala\\nval flightData2015 = spark\\n.read\\n.option(“inferSchema”, “true”)\\n.option(“header”, “true”)\\n.csv(“/mnt/defg/flight-data/csv/2015-summary.csv”)\\n%python\\nflightData2015 = spark\\\\\\n.read\\\\\\n.option(“inferSchema”, “true”)\\\\\\n.option(“header”, “true”)\\\\\\n.csv(“/mnt/defg/flight-data/csv/2015-summary.csv”)\\n \\nEach of these DataFrames (in Scala and Python) each have a set of columns with an unspecified number of rows. \\nThe reason the number of rows is “unspecified” is because reading data is a transformation, and is therefore a lazy \\noperation. Spark only peeked at the data to try to guess what types each column should be.\\nFigure \\nJSON \\nfile\\nflightData2015\\nDataFrame\\nread\\nLazy\\nEager\\ntake (2)\\nArray( Row(...),Row(...))\\n13\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 12, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='If we perform the take action on the DataFrame, we will be able to see the same results that we saw before when we \\nused the command line.\\nflightData2015.take(3)\\nLet’s specify some more transformations! Now we will sort our data according to the count column which is an  \\ninteger type.\\nnote\\nRemember, the sort does not modify the DataFrame. We use the sort is a transformation that returns a new \\nDataFrame by transforming the previous DataFrame. Let’s take a look at this transformation as an illustration.\\nNothing will happen to the data when we call this sort because it’s just a transformation. However, we can see that \\nSpark is building up a plan for how it will execute this across the cluster by looking at the explain plan. We can call \\nexplain on any DataFrame object to see the DataFrame’s lineage.\\nflightData2015.sort(“count”).explain()\\nCongratulations, you’ve just read your first explain plan! Explain plans are a bit arcane, but with a bit of practice it \\nbecomes second nature. Explain plans can be read from top to bottom, the top being the end result and the bottom \\nFigure 7:\\nJSON file\\nRead\\nDataFrame\\nSort\\nDataFrame\\n14\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 13, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='being the source(s) of data. In our case, just take a look at the first keywords. You will see “sort”, “exchange”, and \\n“FileScan”. That’s because the sort of our data is actually a wide dependency because rows will have to be compared \\nwith one another. Don’t worry too much about understanding everything about explain plans, they can just be helpful \\ntools for debugging and improving your knowledge as you progress with Spark.\\nNow, just like we did before, we can specify an action in order to kick off this plan.\\nflightData2015.sort(“count”).take(2)\\nThis will get us the sorted results, as expected. This operation is illustrated in the following image. \\nThe logical plan of transformations that we build up defines a lineage for the DataFrame so that at any given point in \\ntime Spark knows how to recompute any partition by performing all of the operations it had before on the same input \\ndata. This sits at the heart of Spark’s programming model, functional programming where the same inputs always \\nresult in the same outputs when the transformations on that data stay constant.\\nNow that we performed this action, remember that we can navigate to the Spark UI (port 4040) and see the \\ninformation about this jobs stages and tasks.\\nDataFrames and SQL\\nWe worked through a simple example in the previous example, let’s now work through a more complex example and \\nfollow along in both DataFrames and SQL. For your purposes, DataFrames and SQL, in Spark, are the exact same \\nthing. You can express your business logic in either language and Spark will compile that logic down to an underlying \\nplan (that we see in the explain plan) before actually executing your code. Spark SQL allows you as a user to register \\nany DataFrame as a table or view (a temporary table) and query it using pure SQL. There is no performance difference \\nbetween writing SQL queries or writing DataFrame code, they both “compile” to the same underlying plan that we \\nspecify in DataFrame code.\\nFigure \\nJSON file\\nRead\\nDataFrame\\nSort\\ntake(2)\\nDataFrame\\nArray(...)\\n15\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 14, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Any DataFrame can be made into a table or view with one simple method call.\\n%scala\\nflightData2015.createOrReplaceTempView(“flight_data_2015”)\\n%python\\nflightData2015.createOrReplaceTempView(“flight_data_2015”)\\nNow we can query our data in SQL. To execute a SQL query, we’ll use the spark.sql function (remember spark is \\nour SparkSession variable?) that conveniently, returns a new DataFrame. While this may seem a bit circular in logic \\n- that a SQL query against a DataFrame returns another DataFrame, it’s actually quite powerful. As a user, you can \\nspecify transformations in the manner most convenient to you at any given point in time and not have to trade any \\nefficiency to do so! To understand that this is happening, let’s take a look at two explain plans.\\n%scala\\nval sqlWay = spark.sql(“””\\nSELECT DEST_COUNTRY_NAME, count(1)\\nFROM flight_data_2015\\nGROUP BY DEST_COUNTRY_NAME\\n“””)\\nval dataFrameWay = flightData2015\\n.groupBy(‘DEST_COUNTRY_NAME)\\n.count()\\nsqlWay.explain\\ndataFrameWay.explain\\n%python\\nsqlWay = spark.sql(“””\\nSELECT DEST_COUNTRY_NAME, count(1)\\nFROM flight_data_2015\\nGROUP BY DEST_COUNTRY_NAME\\n“””)\\n16\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 15, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='dataFrameWay = flightData2015\\\\\\n.groupBy(“DEST_COUNTRY_NAME”)\\\\\\n.count()\\nsqlWay.explain()\\ndataFrameWay.explain()\\nWe can see that these plans compile to the exact same underlying plan!\\nTo reinforce the tools available to us, let’s pull out some interesting statistics from our data. One thing to understand \\nis that DataFrames (and SQL) in Spark already have a huge number of manipulations available. There are hundreds \\nof functions that you can leverage and import to help you resolve your big data problems faster. We will use the max \\nfunction, to find out what the maximum number of flights to and from any given location are. This just scans each \\nvalue in relevant column the DataFrame and sees if it’s bigger than the previous values that have been seen. This is a \\ntransformation, as we are effectively filtering down to one row. Let’s see what that looks like.\\n// scala or python\\nspark.sql(“SELECT max(count) from flight_data_2015”).take(1)\\n%scala\\nimport org.apache.spark.sql.functions.max\\nflightData2015.select(max(“count”)).take(1)\\n%python\\nfrom pyspark.sql.functions import max\\nflightData2015.select(max(“count”)).take(1)\\nGreat, that’s a simple example. Let’s perform something a bit more complicated and find out the top five destination \\ncountries in the data set? This is a our first multi-transformation query so we’ll take it step by step. We will start with a \\nfairly straightforward SQL aggregation.\\n17\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 16, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nval maxSql = spark.sql(“””\\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\\nFROM flight_data_2015\\nGROUP BY DEST_COUNTRY_NAME\\nORDER BY sum(count) DESC\\nLIMIT 5\\n“””)\\nmaxSql.collect()\\n%python\\nmaxSql = spark.sql(“””\\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\\nFROM flight_data_2015\\nGROUP BY DEST_COUNTRY_NAME\\nORDER BY sum(count) DESC\\nLIMIT 5\\n“””)\\nmaxSql.collect()\\nNow let’s move to the DataFrame syntax that is semantically similar but slightly different in implementation and \\nordering. But, as we mentioned, the underlying plans for both of them are the same. Let’s execute the queries and see \\ntheir results as a sanity check.\\n%scala\\nimport org.apache.spark.sql.functions.desc\\nflightData2015\\n.groupBy(“DEST_COUNTRY_NAME”)\\n.sum(“count”)\\n.withColumnRenamed(“sum(count)”, “destination_total”)\\n.sort(desc(“destination_total”))\\n.limit(5)\\n.collect()\\n18\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 17, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%python\\nfrom pyspark.sql.functions import desc\\nflightData2015\\\\\\n.groupBy(“DEST_COUNTRY_NAME”)\\\\\\n.sum(“count”)\\\\\\n.withColumnRenamed(“sum(count)”, “destination_total”)\\\\\\n.sort(desc(“destination_total”))\\\\\\n.limit(5)\\\\\\n.collect()\\nNow there are 7 steps that take us all the way back to the source data. You can see this in the explain plan on those \\nDataFrames. Illustrated below are the set of steps that we perform in “code”. The true execution plan (the one visible \\nin explain) will differ from what we have below because of optimizations in physical execution, however the llustration \\nis as good of a starting point as any. This execution plan is a directed acyclic graph (DAG) of transformations, each \\nresulting in a new immutable DataFrame, on which we call an action to generate a result.\\nThe first step is to read in the data. We defined the DataFrame previously but, as a reminder, Spark does not actually \\nread it in until an action is called on that DataFrame or one derived from the original DataFrame.\\nThe second step is our grouping, technically when we call groupBy we end up with a RelationalGroupedDataset \\nwhich is a fancy name for a DataFrame that has a grouping specified but needs a user to specify an aggregation before \\nit can be queried further. We can see this by trying to perform an action on it (which will not work). We still haven’t \\nperformed any computation (besides relational algebra) - we’re simply passing along information about the layout of \\nFigure 9:\\nCSV file\\nRead\\nDataFrame\\nDataFrame\\nDataFrame\\ngroupBy\\nSort\\nDataFrame\\nCollect\\nLimit\\nGrouped Dataset\\nDataFrame\\nArray(...)\\nOur result\\nColumn \\nrenamed\\n19\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 18, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='the data.\\nTherefore the third step is to specify the aggregation. Let’s use the sum aggregation method. This takes as input \\na column expression or simply, a column name. The result of the sum method call is a new dataFrame. You’ll see \\nthat it has a new schema but that it does know the type of each column. It’s important to reinforce (again!) that no \\ncomputation has been performed. This is simply another transformation that we’ve expressed and Spark is simply \\nable to trace the type information we have supplied.\\nThe fourth step is a simple renaming, we use the withColumnRenamed method that takes two arguments, the \\noriginal column name and the new column name. Of course, this doesn’t perform computation - this is just another \\ntransformation! \\nThe fifth step sorts the data such that if we were to take results off of the top of the DataFrame, they would be the \\nlargest values found in the destination_total column.\\nYou likely noticed that we had to import a function to do this, the desc function. You might also notice that desc \\ndoes not return a string but a Column. In general, many DataFrame methods will accept Strings (as column names) or \\nColumn types or expressions. Columns and expressions are actually the exact same thing.\\nThe final step is just a limit. This just specifies that we only want five values. This is just like a filter except that it filters \\nby position (lazily) instead of by value. It’s safe to say that it basically just specifies a DataFrame of a certain size.\\nThe last step is our action! Now we actually begin the process of collecting the results of our DataFrame above and \\nSpark will give us back a list or array in the language that we’re executing. Now to reinforce all of this, let’s look at the \\nexplain plan for the above query.\\n%scala\\nflightData2015\\n.groupBy(“DEST_COUNTRY_NAME”)\\n.sum(“count”)\\n.withColumnRenamed(“sum(count)”, “destination_total”)\\n.sort(desc(“destination_total”))\\n.limit(5)\\n.explain()\\n%python\\nflightData2015\\\\\\n.groupBy(“DEST_COUNTRY_NAME”)\\\\\\n.sum(“count”)\\\\\\n.withColumnRenamed(“sum(count)”, “destination_total”)\\\\\\n20\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 19, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='.sort(desc(“destination_total”))\\\\\\n.limit(5)\\\\\\n.explain()\\n== Physical Plan ==\\nTakeOrderedAndProject(limit=5, orderBy=[destination_total#16194L DESC], \\noutput=[DEST_COUNTRY_\\n+- *HashAggregate(keys=[DEST_COUNTRY_NAME#7323], functions=[sum(count#7325L)])\\n +- Exchange hashpartitioning(DEST_COUNTRY_NAME#7323, 5)\\n  +- *HashAggregate(keys=[DEST_COUNTRY_NAME#7323], functions=[partial_   \\nsum(count#7325L)])\\n     +- InMemoryTableScan [DEST_COUNTRY_NAME#7323, count#7325L]\\n     +- InMemoryRelation [DEST_COUNTRY_NAME#7323, ORIGIN_COUNTRY_NAME#7324, count#\\n        +- *Scan csv [DEST_COUNTRY_NAME#7578,ORIGIN_COUNTRY_NAME#7579,count#7580L]\\n \\nWhile this explain plan doesn’t match our exact “conceptual plan” all of the pieces are there. You can see the limit \\nstatement as well as the orderBy (in the first line). You can also see how our aggregation happens in two phases, in \\nthe partial_sum calls. This is because summing a list of numbers is commutative and Spark can perform the sum, \\npartition by partition. Of course we can see how we read in the DataFrame as well.\\nNaturally, we don’t always have to collect the data. We can also write it out to any data source that Spark supports. \\nFor instance, let’s say that we wanted to store the information in a database, we could write these results out to JDBC. \\nWe could also write them out to a new file.\\nThis chapter introduces the basics of Spark. We talked about transformations and actions, how Spark lazily executes \\na DAG of transformations in order to optimize the execution plan on DataFrames. We talked about how data is \\norganized into partitions and set the stage for working with more complex transformations. The next chapter will help \\nshow you around the vast Spark ecosystem. We will see some more advanced concepts and tools that are available \\nin Spark, from Streaming to Machine Learning, to explore all that Spark has to offer in addition to the features and \\nconcepts covered in this chapter.\\n21\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 20, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='A Tour of Spark’s Toolset\\nIn the previous chapter we introduced Spark’s core concepts like transformations and actions. These simple \\nconceptual building blocks have created an entire ecosystem of tools that leverage Spark for a variety of different \\ntasks, from graph analysis and machine learning to streaming and integrations. These conceptual building blocks \\nare the tools that you will use throughout your time with Spark and allow you to understand how Spark executes \\nyour workloads. \\nLet’s take a look at the following illustration of Spark’s functionality.\\nIn the first chapter, we covered most of Spark’s Structured APIs, especially DataFrames and SQL. This chapter will \\ntouch on all the other pieces of functionality in Spark’s toolset. It should come as no surprise that this diagram \\neffectively mirrors the individual parts of the book except that we start at the heart with DataFrames and SQL and \\nthen work our way out to the different higher level building blocks.\\nThis chapter will aim to share a number of things that you can do with Spark at a higher level.\\nWe’ll start by covering a bit more about Spark’s core functionality and touch on the lower level APIs. We will then \\ncover Structured Streaming before performing some advanced analytics and looking at Spark’s package ecosystem. \\nThe entire book covers these topics in depth, the goal of this chapter is simply the tour. Allowing you as a user to \\n“choose your own adventure” and dive into what interests you most next.\\nFigure \\nStructured APIs\\nDataFrames\\nSQL \\nDatasets \\nStructured\\nstreaming \\nAdvanced analytics\\nML graph \\nDeep learning \\nEcosystem \\n+\\nPackages\\nLow level APIs\\nDistributed variables \\nRDDs \\n22\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 21, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Datasets\\nIf you followed the development of Spark in the past year, you will undoubtedly heard of Datasets. DataFrames are \\na distributed collection of objects of type Row (more in the next chapter) but Spark also allows JVM users to create \\ntheir own objects (via case classes or java beans) and manipulate them using function programming concepts. For \\ninstance, rather than creating a range and manipulating it via SQL or DataFrames, we can manipulate it just as we \\nmight manipulate a local scala collection. We can map over the values with a user defined function, and convert it into \\na new arbitrary case class object. \\nThe amazing thing about Datasets is that we can use them only when we need or want to. For instance, in the \\nfollowing example I’ll define my own object and manipulate it via arbitrary map and filter functions. Once we’ve \\nperformed our manipulations, Spark can automatically turn it back into a DataFrame and we can manipulate it \\nfurther using the hundreds of functions that Spark includes. This makes it easy to drop down to lower level, type \\nsecure coding when necessary, and move higher up to SQL for more rapid analysis. We cover this material extensively \\nin the next part of this book, but this ability to manipulate arbitrary case classes with arbitrary functions makes \\nexpressing business logic simple.\\ncase class ValueAndDouble(value:Long, valueDoubled:Long)\\nspark.range(2000)\\n.map(value => ValueAndDouble(value, value * 2))\\n.filter(vAndD => vAndD.valueDoubled % 2 == 0)\\n.where(“value % 3 = 0”)\\n.count()\\nCaching Data for Faster Access\\nWe discussed wide and narrow transformations in the previous chapter and how Spark automatically spills to disk \\nwhen we perform a shuffle. However, sometimes we’re going to access a DataFrame multiple times in the same data \\nflow and we want to avoid performing expensive joins over and over again. Imagine that we were to access the initial \\ndata set numerous times. A smart thing to do would be to cache the data in memory since we repeatedly access this \\ndata. Now remember that a DataFrame always has to go back to a robust data source as its origin, caching is simply a \\n23\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 22, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='way to create a new “origin” along the way. Let’s illustrate this with a drawing.\\nYou’ll see here that we have our “lazily” created DataFrame along with the DataFrames that we will create in the \\nfuture. The problem is that all of our downstream DataFrames share a common parent and they all have to do the \\nwork of creating that same DataFrame. In this case it’s just reading in the raw data but that can be a fairly intensive \\nprocess - especially for large datasets. Let’s just perform this exact example and see how long it takes.\\n%scala\\nval DF1 = spark.read.format(“csv”)\\n.option(“inferSchema”, “true”)\\n.option(“header”, “true”)\\n.load(“/mnt/defg/flight-data/csv/2015-summary.csv”)\\n%python\\nDF1 = spark.read.format(“csv”)\\\\\\n.option(“inferSchema”, “true”)\\\\\\n.option(“header”, “true”)\\\\\\n.load(“/mnt/defg/flight-data/csv/2015-summary.csv”)\\nFigure 2:\\n2015-summary.csv\\nInitial DataFrame\\nGroup by \\ndestination \\nDF2\\nDF3\\nDF4\\nDFS\\nGroup by \\norigin\\nGroup by \\ncount\\nSum(count)\\n24\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 23, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nval DF2 = DF1.groupBy(“DEST_COUNTRY_NAME”).count().collect()\\nval DF3 = DF1.groupBy(“ORIGIN_COUNTRY_NAME”).count().collect()\\nval df4 = DF1.groupBy(“count”).count().collect()\\n%python\\nDF2 = DF1.groupBy(“DEST_COUNTRY_NAME”).count().collect()\\nDF3 = DF1.groupBy(“ORIGIN_COUNTRY_NAME”).count().collect()\\nDF4 = DF1.groupBy(“count”).count().collect()\\nNow on my machine that took about 2.75 seconds. Luckily caching can help speed things up. When we specify a \\nDataFrame to be cached, the first time that Spark reads it in - it will save it to be accessed again later. Then when any \\nother queries come along, they’ll just refer to the one stored in memory as opposed to the original file.\\nRather than having to recompute that initial DataFrame we can save it into memory with the cache method. This will \\ncut down on the rest of the computation because Spark will already have the data stored in memory.\\n%scala\\nDF1.cache()\\nDF1.count()\\n%python\\nDF1.cache()\\nFigure 3:\\nRaw data\\nDF1\\nDF1\\nCache\\n25\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 24, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='DF1.count()\\n%scala\\nval DF2 = DF1.groupBy(“DEST_COUNTRY_NAME”).count().collect()\\nval DF3 = DF1.groupBy(“ORIGIN_COUNTRY_NAME”).count().collect()\\nval df4 = DF1.groupBy(“count”).count().collect()\\n%python\\nDF2 = DF1.groupBy(“DEST_COUNTRY_NAME”).count().collect()\\nDF3 = DF1.groupBy(“ORIGIN_COUNTRY_NAME”).count().collect()\\ndf4 = DF1.groupBy(“count”).count().collect()\\nWe can see that this cuts the time by more than half! This may not seem that wild but picture a large data set or one \\nthat requires a lot of computation in order to create (not just reading in a file). The savings can be immense. It’s also \\ngreat for iterative machine learning workloads because they’ll often have to access the same data a number of times \\nwhich we’ll see shortly.\\nNow sometimes our data may be too large to fit into memory. That’s why there’s also the persist method and \\na number of options for us to save data to the disk of our machine. We discuss this in Part IV of the book when we \\ndiscuss optimizations and tuning.\\nStructured Streaming\\nStructured Streaming became Production Ready in Spark 2.2. Structured streaming allows you to take the same \\noperations that you perform in batch mode and perform them in a streaming fashion. This can reduce latency and \\nallow for incremental processing. The best thing about Structured Streaming is that it allows you to rapidly and \\nquickly get value out of streaming systems with simple switches, it also makes it easy to reason about because you \\ncan write your batch job as a way to prototype it and then you can convert it to streaming job. The way all of this \\nworks is by incrementally processing that data.\\nLet’s walk through a simple example of how easy it is to get started with Structured Streaming. For this we will use an \\nretail dataset. One that has specific dates and times for us to be able to use. When we list the directory, you will notice \\nthat there are a variety of different files in there.\\n%fs ls /mnt/defg/retail-data/by-day/\\nThe first thing that you’ll notice is that we’ve got a lot of different files. This is to process. Now this is retail data so \\n26\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 25, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='imagine that these are being produced by retail stores and sent to a location where they will be read by our Structured \\nStreaming job. Now in order to ground this, let’s first analyze the data as a static dataset and create a DataFrame to do \\nso. We’ll also create a schema so that when we read the data in later we don’t have to infer it.\\n%scala\\nval staticDataFrame = spark.read.format(“csv”)\\n.option(“header”, “true”)\\n.option(“inferSchema”, “true”)\\n.load(“dbfs:/mnt/defg/retail-data/by-day/*.csv”)\\nstaticDataFrame.createOrReplaceTempView(“retail_data”)\\nval staticSchema = staticDataFrame.schema\\n%python\\nstaticDataFrame = spark.read.format(“csv”)\\\\\\n.option(“header”, “true”)\\\\\\n.option(“inferSchema”, “true”)\\\\\\n.load(“dbfs:/mnt/defg/retail-data/by-day/*.csv”)\\nstaticDataFrame.createOrReplaceTempView(“retail_data”)\\nstaticSchema = staticDataFrame.schema\\nNow since we’re working with time series data it’s worth mentioning how we might go along grouping and \\naggregating our data. In this example we’ll take a look at the largest sale hours where a given customer (identified by \\nCustomerId) makes a large purchase. For example, let’s add a total cost column and see on what days a customer \\nspent the most. \\nimport org.apache.spark.sql.functions.{window, column, desc, col}\\nstaticDataFrame\\n.selectExpr(\\n“CustomerId”,\\n“(UnitPrice * Quantity) as total_cost”,\\n“InvoiceDate”)\\n.groupBy(\\n27\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 26, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='col(“CustomerId”), window(col(“InvoiceDate”), “1 day”))\\n.sum(“total_cost”)\\n.orderBy(desc(“sum(total_cost)”))\\n.take(5)\\n%python\\nfrom pyspark.sql.functions import window, column, desc, col\\nstaticDataFrame\\\\\\n.selectExpr(\\n“CustomerId”,\\n“(UnitPrice * Quantity) as total_cost” ,\\n“InvoiceDate” )\\\\\\n.groupBy(\\ncol(“CustomerId”), window(col(“InvoiceDate”), “1 day”))\\\\\\n.sum(“total_cost”)\\\\\\n.orderBy(desc(“sum(total_cost)”))\\\\\\n.take(5)\\n%sql\\nSELECT\\nsum(total_cost),\\nCustomerId,\\nto_date(InvoiceDate)\\nFROM\\n(SELECT\\nCustomerId,\\n(UnitPrice * Quantity) as total_cost,\\nInvoiceDate\\nFROM\\nretail_data)\\nGROUP BY\\nCustomerId, to_date(InvoiceDate)\\nORDER BY\\nsum(total_cost) DESC\\nThat’s the static DataFrame version, there shouldn’t be any big surprises in there if you’re familiar with the syntax. \\nNow we’ve seen how that works, let’s take a look at the streaming code! You’ll notice that very little actually changes \\nabout our code. The biggest change is that we used readStream instead of read, additionally you’ll notice \\n28\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 27, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='maxFilesPerTrigger option which simply specifies the number of files we should read in at once. This is to make \\nour demonstration more “streaming” and in a production scenario this would be omitted.\\nNow since you’re likely running this in local mode, it’s a good practice to set the number of shuffle partitions to \\nsomething that’s going to be a better fit for local mode. This configuration simple specifies the number of partitions \\nthat should be created after a shuffle, by default the value is 200 but since there aren’t many executors on my local \\nmachine it’s worth reducing this to five.\\nspark.conf.set(“spark.sql.shuffle.partitions”, “5”)\\nval streamingDataFrame = spark.readStream\\n.schema(staticSchema)\\n.option(“maxFilesPerTrigger”, 1)\\n.format(“csv”)\\n.option(“header”, “true”)\\n.load(“dbfs:/mnt/defg/retail-data/by-day/*.csv”)\\n%python\\nstreamingDataFrame = spark.readStream\\\\\\n.schema(staticSchema)\\\\\\n.option(“maxFilesPerTrigger”, 1)\\\\\\n.format(“csv”)\\\\\\n.option(“header”, “true”)\\\\\\n.load(“dbfs:/mnt/defg/retail-data/by-day/*.csv”)\\nNow we can see the DataFrame is streaming.\\nstreamingDataFrame.isStreaming // returns true\\nThis is still a lazy operation, so we will need to call a streaming action to start the execution of this data flow. Let’s run \\nthe exact same query that we were running on our static dataset except now what will happen is that Spark will only \\nread in one file at a time.\\n29\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 28, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nval purchaseByCustomerPerHour = streamingDataFrame\\n.selectExpr(\\n“CustomerId”,\\n“(UnitPrice * Quantity) as total_cost”,\\n“InvoiceDate”)\\n.groupBy(\\n$”CustomerId”, window($”InvoiceDate”, “1 day”))\\n.sum(“total_cost”)\\n%python\\npurchaseByCustomerPerHour = streamingDataFrame\\\\\\n.selectExpr(\\n“CustomerId”,\\n“(UnitPrice * Quantity) as total_cost” ,\\n“InvoiceDate” )\\\\\\n.groupBy(\\ncol(“CustomerId”), window(col(“InvoiceDate”), “1 day”))\\\\\\n.sum(“total_cost”)\\nNow let’s kick off the stream! We’ll write it out to an in-memory table that we will update after each trigger. In this \\ncase, each trigger is based on an individual file (the read option that we set). Spark will mutate the data in the in-\\nmemory table such that we will always have the highest value.\\n%scala\\npurchaseByCustomerPerHour.writeStream\\n.format(“memory”) // memory = store in-memory table\\n.queryName(“customer_purchases”) // counts = name of the in-memory table\\n.outputMode(“complete”) // complete = all the counts should be in the table\\n.start()\\n30\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 29, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%python\\npurchaseByCustomerPerHour.writeStream\\\\\\n.format(“memory”)\\\\\\n.queryName(“customer_purchases”)\\\\\\n.outputMode(“complete”)\\\\\\n.start()\\nNow we can run queries against this table. Note to take a minute before doing so, this will allow the values to change  \\nover time.\\n%scala\\nspark.sql(“””\\nSELECT *\\nFROM customer_purchases\\nORDER BY `sum(total_cost)` DESC\\n“””)\\n.take(5)\\n%python\\nspark.sql(“””\\nSELECT *\\nFROM customer_purchases\\nORDER BY `sum(total_cost)` DESC\\n“””)\\\\\\n.take(5)\\nYou’ll notice that as we read in more data - the composition of our table changes! With each file the results may or \\nmay not be changing based on the data. Naturally since we’re grouping customers we hope to see an increase in the \\ntop customer purchase amounts over time (and do for a period of time!). Another option you can use is to just simply \\nwrite the results out to the console.\\n// another option\\n// purchaseByCustomerPerHour.writeStream\\n// .format(“console”)\\n// .queryName(“customer_purchases_2”) // counts = name of the in-memory table\\n31\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 30, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='// .outputMode(“complete”) // complete = all the counts should be in the table\\n// .start()\\nNeither of these streaming methods should be used in production but they do make for convenient demonstration of \\nStructured Streaming’s power. Notice how this window is built on event time as well, not the time at which the data \\nSpark processes the data. This was one of the shortcoming of Spark Streaming. We cover Structured Streaming in \\ndepth in Part V of this book.\\nMachine Learning and Advanced Analytics\\nAnother popular aspect of Spark is its ability to perform large scale machine learning with a built-in library of machine \\nlearning algorithms called MLlib. MLlib allows for preprocessing, munging, training of models, and making predictions \\nat scale on data. You can even use models trained in MLlib to make predictions in Strucutred Streaming. Spark \\nprovides a sophisticated machine learning API for performing a variety of machine learning tasks, from classification \\nto regression and clustering. To demonstrate this functionality, we will perform some basic clustering on our data.\\nBOX What is K-Means? K-means is a clustering algorithm where “K” centers are randomly assigned within the \\ndata. The points closest to that point are then “assigned” to a class and the center of the assigned points is \\ncomputed. This center point is called the centroid. We then label the points closest to that centroid, to the \\ncentroid’s class, and shift the centroid to the new center of that cluster of points. We repeat this process for a \\nfinite set of iterations or until convergence (our center points stop changing).\\nWe will perform this task on some fairly raw data. This will allow us to demonstrate how we build up a series of \\ntransformations in order to get our data into the correct format. From there we can actually train our model and then \\nserve predictions.\\nstaticDataFrame.printSchema()\\nMachine learning algorithms in MLlib, for the most part, require data to be represented as numerical values. Our \\ncurrent data is represented by a variety of different types including timestamps, integers, and strings. Therefore \\nwe need to transform this data into some numerical representation. In this instance, we will use several DataFrame \\ntransformations to manipulate our date data.\\n32\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 31, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nimport org.apache.spark.sql.functions.date_format\\nval preppedDataFrame = staticDataFrame\\n.na.fill(0)\\n.withColumn(“day_of_week”, date_format($”InvoiceDate”, “EEEE”))\\n.coalesce(5)\\n%python\\nfrom pyspark.sql.functions import date_format, col\\npreppedDataFrame = staticDataFrame\\\\\\n.na.fill(0)\\\\\\n.withColumn(“day_of_week”, date_format(col(“InvoiceDate”), “EEEE”))\\\\\\n.coalesce(5)\\nNow we are also going to need to split our data into training and test sets. In this instance we are going to do this  \\nmanually by the date that a certain purchase occurred however we could also leverage MLlib’s transformation APIs to \\ncreate a  training and test set via train validation splits or cross validation. These topics are covered extensively in Part \\nsix of this book.\\n%scala\\nval trainDataFrame = preppedDataFrame\\n.where(“InvoiceDate < ‘2011-07-01’”)\\nval testDataFrame = preppedDataFrame\\n.where(“InvoiceDate >= ‘2011-07-01’”)\\n%python\\ntrainDataFrame = preppedDataFrame\\\\\\n.where(“InvoiceDate < ‘2011-07-01’”)\\ntestDataFrame = preppedDataFrame\\\\\\n.where(“InvoiceDate >= ‘2011-07-01’”)\\nNow that we prepared our data, let’s split it into a training and test set. Since this is a time-series set of data, we will \\nsplit by an arbitrary date in the dataset. While this may not be the optimal split for our training and test, for the intents \\nand purposes of this example it will work just fine. We’ll see that this splits our dataset roughly in half.\\n33\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 32, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='trainDataFrame.count()\\ntestDataFrame.count()\\nNow these transformations are DataFrame transformations, covered extensively in part two of this book. Spark’s MLlib \\nalso provides a number of transformations that allow us to automate some of our general transformations. One such \\ntransformer is a StringIndexer.\\n%scala\\nimport org.apache.spark.ml.feature.StringIndexer\\nval indexer = new StringIndexer()\\n.setInputCol(“day_of_week”)\\n.setOutputCol(“day_of_week_index”)\\n%python\\nfrom pyspark.ml.feature import StringIndexer\\nindexer = StringIndexer()\\\\\\n.setInputCol(“day_of_week”)\\\\\\n.setOutputCol(“day_of_week_index”)\\nThis will turn our days of weeks into corresponding numerical values. For example, Spark may represent Saturday \\nas 6 and Monday as 1. However with this numbering scheme, we are implicitly stating that Saturday is greater than \\nMonday (by pure numerical values). This is obviously incorrect. Therefore we need to use a OneHotEncoder to \\nencode each of these values as their own column. These boolean flags state whether that day of week is the relevant \\nday of the week.\\n%scala\\nimport org.apache.spark.ml.feature.OneHotEncoder\\nval encoder = new OneHotEncoder()\\n.setInputCol(“day_of_week_index”)\\n.setOutputCol(“day_of_week_encoded”)\\n%python\\n34\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 33, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='from pyspark.ml.feature import OneHotEncoder\\nencoder = OneHotEncoder()\\\\\\n.setInputCol(“day_of_week_index”)\\\\\\n.setOutputCol(“day_of_week_encoded”)\\nEach of these will result in a set of columns that we will “assemble” into a vector. All machine learning algorithms in \\nSpark take as input a Vector type, which must be a set of numerical values.\\n%scala\\nimport org.apache.spark.ml.feature.VectorAssembler\\nval vectorAssembler = new VectorAssembler()\\n.setInputCols(Array(“UnitPrice”, “Quantity”, “day_of_week_encoded”))\\n.setOutputCol(“features”)\\n%python\\nfrom pyspark.ml.feature import VectorAssembler\\nvectorAssembler = VectorAssembler()\\\\\\n.setInputCols([“UnitPrice”, “Quantity”, “day_of_week_encoded”])\\\\\\n.setOutputCol(“features”)\\nWe can see that we have 4 key features, the price, the quantity, and the day of week. Now we’ll set this up into a \\npipeline so any future data we need to transform can go through the exact same process.\\n%scala\\nimport org.apache.spark.ml.Pipeline\\nval transformationPipeline = new Pipeline()\\n.setStages(Array(indexer, encoder, vectorAssembler))\\n%python\\nfrom pyspark.ml import Pipeline\\n35\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 34, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='transformationPipeline = Pipeline()\\\\\\n.setStages([indexer, encoder, vectorAssembler])\\nNow preparing for training is a two step process. We first need to fit our transformers to this dataset. We cover this in \\ndepth, but basically our StringIndexer needs to know how many unique values there are to be index. Once those \\nexist, encoding is easy but Spark must look at all the distinct values in the column to be indexed in order to store \\nthose values later on.\\n%scala\\nval fittedPipeline = transformationPipeline.fit(trainDataFrame)\\n%python\\nfittedPipeline = transformationPipeline.fit(trainDataFrame)\\nOnce we fit the training data, we are now create to take that fitted pipeline and use it to transform all of our data in a \\nconsistent and repeatable way.\\n%scala\\nval transformedTraining = fittedPipeline.transform(trainDataFrame)\\n%python\\ntransformedTraining = fittedPipeline.transform(trainDataFrame)\\nAt this point, it’s worth mentioning that we could have included our model training in our pipeline. We chose not to \\nin order to demonstrate a use case for caching the data. At this point, we’re going to perform some hyperparameter \\ntuning on the model, since we do not want to repeat the exact same transformations over and over again, we’ll \\ninstead cache our training set. This is worth putting it into memory because that will allow us to efficiently, and \\nrepeatedly access it in an already transformed state. If you’re curious to see how much of a difference this makes, \\nskip this line and run the training without caching the data. Then try it after caching, you’ll see the results are (very) \\nsignificant.\\ntransformedTraining.cache()\\n36\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 35, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Now we have a training set, now it’s time to train the model. First we’ll import the relevant model that we’d like to use.\\n%scala\\nimport org.apache.spark.ml.clustering.KMeans\\nval kmeans = new KMeans()\\n.setK(20)\\n.setSeed(1L)\\n%python\\nfrom pyspark.ml.clustering import KMeans\\nkmeans = KMeans()\\\\\\n.setK(20)\\\\\\n.setSeed(1L)\\nIn Spark, training machine learning models is a two phase process. First we initialize an untrained model, then we \\ntrain it. There are always two types for every algorithm in MLlib’s DataFrame API. The algorithm Kmeans and then the \\ntrained version which is a KMeansModel.\\nAlgorithms and models in MLlib’s DataFrame API share roughly the same interface that we saw above with our \\npreprocessing transformers like the StringIndexer. This should come as no surprise because it makes training an entire \\npipeline (which includes the model) simple. In our case we want to do things a bit more step by step, so we chose to \\nnot do this at this point.\\n%scala\\nval kmModel = kmeans.fit(transformedTraining)\\n%python\\nkmModel = kmeans.fit(transformedTraining)\\nWe can see the resulting cost at this point. Which is quite high, that’s likely because we didn’t necessary scale our data  \\nor transform.\\n37\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 36, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='kmModel.computeCost(transformedTraining)\\n%scala\\nval transformedTest = fittedPipeline.transform(testDataFrame)\\n%python\\ntransformedTest = fittedPipeline.transform(testDataFrame)\\nkmModel.computeCost(transformedTest)\\nNaturally we could continue to improve this model, layering more preprocessing as well as performing \\nhyperparameter tuning to ensure that we’re getting a good model. We leave that discussion for Part VI of this book, \\nwhere we discuss MLlib in depth.\\nSpark’s Ecosystem and Packages\\nArguably one of the best parts about Spark is the ecosystem of packages and tools that the community has created. \\nSome of these tools even move into open source Spark as they mature and become widely used. The list of packages \\nis rather large at over 300 at the time of this writing and more are added frequently. A consolidated list of packages \\ncan be found at https://spark-packages.org/ where any user can publish to this package repository and there are \\nnumerous others that are not included.\\nThe thing that brings together all Spark packages is that they allow for engineers to create optimized versions of Spark \\nfor particular applications. GraphFrames is a perfect example, it makes Graph Analysis available on Spark’s Structured \\nAPIs in ways that are much easier to use, and cross language, in a way that the lower level APIs do not support. There \\nare numerous other packages include many machine learning and deep learning packages that leverage Spark as the \\ncore and extend the functionality.\\nBeyond these advanced analytics applications, packages exist to solve problems in particular verticals. Healthcare \\nand genomics have seen a particularly large surge in opportunity for big data applications. For example, the ADAM \\nProject leverages unique, internal optimizations to Spark’s Catalyst engine to provide a scalable API & CLI for genome \\nprocessing. Another package Hail is an opensource, scalable framework for exploring and analyzing genomic data. \\nStarting from sequencing or microarray data in VCF and other formats. Hail provides extremely sophisticated domain \\nrelevant transformations to enable efficient analysis gigabyte-scale data on a laptop or terabyte-scale data on cluster.\\nThere are countless other examples of the community coming together and creating projects to solve large scale \\nproblems and we just wanted to discuss some of these projects and provide a short example of GraphFrames, \\nsomething that you’re likely able to apply to data right away.\\n38\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 37, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='GraphFrames\\nThis section of the chapter will review a popular package, Graph- Frames, for advanced graph analytics. \\nYou can see the GraphFrames package in the Spark-Packages repository. The version used here is \\ngraphframes:graphframes:0.3.0-spark2.0-s_2.10.\\nGraph Analysis with GraphFrames\\nWhile a complete primer or graph analysis is out of scope for this chapter, the general idea is that graph theory and \\nprocessing are about defining relationships between different nodes and edges. Nodes or vertices are the units while \\nedges are the relationships that are defined between those. This object-relationship-object is a common way of \\nstructing problems and GraphFrames makes it very easy to get started.\\nSetup & Data\\nPrior to reading in some data we’re going to need to install the GraphFrames Library. If you’re running this on \\nDatabricks, you should do this by following this guide. Be sure to follow the directions for specifying the maven \\ncoordinates. If you’re running this from the command line, you’ll want to specify the dependency when you launch \\nthe console.\\n./bin/spark-shell --packages graphframes:graphframes:0.5.0-spark2.1-s_2.11\\n%scala\\nval bikeStations = spark.read.format(“csv”)\\n.option(“header”, “true”)\\n.option(“inferSchema”, “true”)\\n.load(“/mnt/defg/bike-data/201508_station_data.csv”)\\nval bikeTrips = spark.read.format(“csv”)\\n.option(“header”, “true”)\\n.option(“inferSchema”, “true”)\\n.load(“/mnt/defg/bike-data/201508_trip_data.csv”)\\n%python\\nbikeStations = spark.read.format(“csv”)\\\\\\n.option(“header”, “true”)\\\\\\n.option(“inferSchema”, “true”)\\\\\\n.load(“/mnt/defg/bike-data/201508_station_data.csv”)\\n39\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 38, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='bikeTrips = spark.read.format(“csv”)\\\\\\n.option(“header”, “true”)\\\\\\n.option(“inferSchema”, “true”)\\\\\\n.load(“/mnt/defg/bike-data/201508_trip_data.csv”)\\nBuilding the Graph\\nNow that we’ve imported our data, we’re going to need to build our graph. To do so we’re going to do two things. We \\nare going to build the structure of the vertices (or nodes) and we’re going to build the structure of the edges. What’s \\nawesome about GraphFrames is that this process is incredibly simple. All that we need to do is rename our name \\ncolumn to id in the Vertices table and the start and end stations to src and dst respectively for our edges tables. These \\nare required naming conventions for vertices and edges in GraphFrames as of the time of this writing.\\n%scala\\nval stationVertices = bikeStations\\n.withColumnRenamed(“name”, “id”)\\n.distinct()\\nval tripEdges = bikeTrips\\n.withColumnRenamed(“Start Station”, “src”)\\n.withColumnRenamed(“End Station”, “dst”)\\n%python\\nstationVertices = bikeStations\\\\\\n.withColumnRenamed(“name”, “id”)\\\\\\n.distinct()\\ntripEdges = bikeTrips\\\\\\n.withColumnRenamed(“Start Station”, “src”)\\\\\\n.withColumnRenamed(“End Station”, “dst”)\\nNow we can build our graph.\\n40\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 39, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nimport org.graphframes.GraphFrame\\nval stationGraph = GraphFrame(stationVertices, tripEdges)\\ntripEdges.cache()\\nstationVertices.cache()\\n%python\\nfrom graphframes import GraphFrame\\nstationGraph = GraphFrame(stationVertices, tripEdges)\\ntripEdges.cache()\\nstationVertices.cache()\\nSpark Packages make it easy to get started, as you saw in the preceding code snippets. Now we will run some out of \\nthe box algorithms to our data to find some interesting information.\\nPageRank\\nBecause GraphFrames build on GraphX (Spark’s original Graph Analysis package), there are a number of built-in \\nalgorithms that we can leverage right away. PageRank is one of the more popular ones popularized by the Google \\nSearch Engine and created by Larry Page. To quote Wikipedia: \\nPageRank works by counting the number and quality of links to a page to determine a rough estimate of how \\nimportant the website is. The underlying assumption is that more important websites are likely to receive more \\nlinks from other websites.\\nWhat’s awesome about this concept is that it readily applies to any graph type structure be them web pages or bike \\nstations. Let’s go ahead and run PageRank on our data.\\n%scala\\nimport org.apache.spark.sql.functions.{desc, col}\\nval ranks = stationGraph.pageRank.resetProbability(0.15).maxIter(10).run()\\nranks.vertices.orderBy(desc(“pagerank”)).take(5)\\n%python\\n41\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 40, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='from pyspark.sql.functions import desc\\nranks = stationGraph.pageRank(maxIter=10).resetProbability(0.15).run()\\nranks.vertices.orderBy(desc(“pagerank”)).take(5)\\nWe can see that the Caltrain stations seem to have significance, and this makes sense. Train stations are natural \\nconnectors and likely one of the most popular uses of these bike share programs to get you from A to B in a way that \\nyou don’t need a car!\\nTrips From Station to Station\\nOne question you might ask is what are the most common destinations in the dataset from location to location. We \\ncan do this by performing a grouping operator and adding the edge counts together. This will yield a new graph except \\neach edge will now be the sum of all of the semantically same edges. Think about it this way: we have a number of \\ntrips that are the exact same from station A to station B, we just want to count those up! \\nIn the below query you’ll see that we’re going to grab the station to station trips that are most common and print out \\nthe top 10.\\n%scala\\nstationGraph\\n.edges\\n.groupBy(“src”, “dst”)\\n.count()\\n.orderBy(desc(“count”))\\n.limit(10)\\n.show()\\n%python\\nstationGraph\\\\\\n.edges\\\\\\n.groupBy(“src”, “dst”)\\\\\\n.count()\\\\\\n.orderBy(desc(“count”))\\\\\\n.limit(10)\\\\\\n.show()\\n42\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 41, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='GraphFrames provides a simple to use interface to get value out or graph structured data right away. Other Spark \\nPackages provide similar functionality, making it simple to leverage hyper-optimized packages for a variety of \\ndifferent data sources, algorithms and applications, and much more.\\nConclusion\\nWe hope this chapter showed you a variety of the different ways that you can apply Spark to your own business and \\ntechnical challenges. Spark’s simple, robust programming model make it easy to apply to a large number of problems \\nand the vast array of packages that have crept up around it created by hundreds of different people are a true \\ntestament to Spark’s ability to apply robustly to a number of business problems and challenges. As the ecosystem and \\ncommunity grows, it’s likely that more and more packages will continue to crop up. We look forward to seeing what \\nthe community has in store!\\nThe rest of this book will provide deeper dives into the product areas in the following image. \\nYou may read the rest of the book anyway that you wish, we find that most people hop from area to area as they hear \\nterminology or want to apply Spark to certain problems they’re facing.\\nFigure \\nStructured APIs\\nDataFrames\\nSQL \\nDatasets \\nStructured\\nstreaming \\nAdvanced analytics\\nML graph \\nDeep learning \\nEcosystem \\n+\\nPackages\\nLow level APIs\\nDistributed variables \\nRDDs \\n43\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 42, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Structured API Overview\\nThis part of the book will be a deep dive into Spark’s Structured APIs. These APIs are core to Spark and likely \\nwhere you’ll spend a lot of time with Spark. The Structured APIs are a way of manipulating all sorts of data, from \\nunstructured log files, to semi-structured CSV files, and highly structured Parquet files. These APIs refer to three core \\ntypes of distributed collection APIs.\\n• Datasets\\n• DataFrames\\n• SQL Views and Tables\\nThis part of the book will cover all the core ideas behind the Structured APIs and how to apply them to your big data \\nchallenges. Although distinct in the book, the vast majority of these user-facing operations apply to both batch as well \\nas streaming computation. Meaning that when you work with the Structured APIs, it should be simple to migrate from \\nbatch to streaming with little to no effort (or the opposite).\\nThe Structured APIs are the fundamental abstraction that you will leverage to write the majority of your data flows. \\nThus far in this book we have taken a tutorial-based approach, meandering our way through much of what Spark \\nhas to offer. In this section, we will perform a deeper dive. This introductory chapter will introduce the fundamental \\nconcepts that you should understand: the typed and untyped APIs (and their differences); how to work with different \\nkinds of data using the structured APIs; and deep dives into different data flows with Spark. The later chapters will \\nprovide operation based information.\\nBOX Before proceeding, let’s review the fundamental concepts and definitions that we covered in the previous \\nsection. Spark is a distributed programming model where the user specifies transformations, which build \\nup a directed acyclic- graph of instructions, and actions, which begin the process of executing that graph of \\ninstructions, as a single job, by breaking it down into stages and tasks to execute across the cluster. The way \\nwe store data on which to perform transformations and actions are DataFrames and Datasets. To create a \\nnew DataFrame or Dataset, you call a transformation. To start computation or convert to native language \\ntypes, you call an action.\\nDataFrames and Datasets\\nIn Section I, we talked all about DataFrames. Spark has two notions of “structured” collections: DataFrames and \\nDatasets. We will touch on the (nuanced) differences shortly but let’s define what they both represent first.\\nTo the user, DataFrames and Datasets are (distributed) table like collections with well-defined rows and columns. \\nEach column must have the same number of rows as all the other columns (although you can use null to specify the \\n44\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 43, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='lack of a value) and columns have type information that must be consistent for every row in the collection. To Spark, \\nDataFrames and Datasets represent immutable, lazily-evaluated plans that specify what operations to apply to \\ndata residing at a location to generate some output. When we perform an action on a DataFrame we instruct Spark \\nto perform the actual transformations and return the result. These represent plans of how to manipulate rows and \\ncolumns to compute the user’s desired result. Let’s go over rows and column to more precisely define those concepts.\\nnote\\nTables and views are basically the same thing as DataFrames. We just execute SQL against them instead of \\nDataFrame code. We cover all of this in the Spark SQL Chapter later on in Part II of this book.\\nIn order to do that,we should talk about schemas, the way we define the types of data we’re storing in this  \\ndistributed collection.\\nSchemas\\nA schema defines the column names and types of a DataFrame. Users can define schemas manually or users can \\nread a schema from a data source (often called schema on read)|. Now that we know what defines DataFrames and \\nDatasets and how they get their structure, via a Schema, let’s see an overview of all of the types.\\nOverview of Structured Spark Types\\nSpark is effectively a programming language of its own. Internally, Spark uses an engine called Catalyst that maintains \\nits own type information through the planning and processing of work. This may seem like overkill, but it doing so, \\nthis opens up a wide variety of execution optimizations that make significant differences. Spark types map directly \\nto the different language APIs that Spark maintains and there exists a lookup table for each of these in each of Scala, \\nJava, Python, SQL, and R. Even if we use Spark’s Structured APIs from Python or R, the majority of our manipulations \\nwill operate strictly on Spark types, not Python types. For example, the below code does not perform addition in Scala \\nor Python, it actually performs addition purely in Spark.\\n%scala\\nval df = spark.range(500).toDF(“number”)\\ndf.select(df.col(“number”) + 10)\\n// org.apache.spark.sql.DataFrame = [(number + 10): bigint]\\n%python\\ndf = spark.range(500).toDF(“number”)\\ndf.select(df[“number”] + 10)\\n# DataFrame[(number + 10): bigint]\\n45\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 44, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='This addition operation happens because Spark will convert an expression expressed in an input language to Spark’s \\ninternal Catalyst representation of that same type information. Now that we’ve clearly defined that Spark maintains \\nits own type information, let’s dive into some of the nuanced differences of the output you may see when you’re \\nworking with Spark.\\nIn essence, within the Structured APIs, there are two more APIs, the “untyped” DataFrames and the “typed” Datasets. \\nTo say that DataFrames are untyped is a bit of a misnomer, they have types but Spark maintains them completely and \\nonly checks whether those types line up to those specified in the schema at runtime. Datasets, on the other hand, \\ncheck whether or not types conform to the specification at compile time. Datasets are only available to JVM based \\nlanguages (Scala and Java) and we specify types with case classes or Java beans.\\nFor the most part, you’re likely to work with DataFrames and we cover DataFrames extensively in this part of the book.\\nTo Spark in Scala, DataFrames are simply Datasets of Type Row. The “Row” type is Spark’s internal representation of \\nits optimized in memory format for computation. This format makes for highly specialized and efficient computation \\nbecause rather than leveraging JVM types which can cause high garbage collection and object instantiation costs, \\nSpark can operate on its own internal format without incurring any of those costs. To Spark in Python, there is no such \\nthing as a Dataset, everything is a DataFrame.\\nnote\\nThis internal format is well covered in numerous Spark talks and for the more general audience we will abstain \\nfrom going into the implementation. For the curious there are some excellent talks by Josh Rosen of Databricks \\nand Herman van Hovell of Databricks about their work in the development of Spark’s Catalyst engine.\\nDefining DataFrames, types, and Schemas is always a mouthful and takes sometime to digest. What most need to \\nknow is that when you’re using DataFrames, you’re leverage Spark’s optimized internal format. This format applies \\nthe same efficiency gains to all of Spark’s language APIs. For those that need strict compile time checking, they should \\nsee the Datasets Chapter at the end of Part II of this book to learn more about them and how to use them.\\nLet’s move onto some more friendly and approachable concepts, columns and rows.\\nColumns\\nFor now, all you need to understand about columns is that they can represent a simple type like an integer or string, \\na complex types like an array or map, or a null value. Spark tracks all of this type information to you and has a variety \\nof ways that you can transform columns. Columns are discussed extensively in the next chapter but for the most part \\nyou can think about Spark Column types as columns in a table.\\n46\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 45, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Rows\\nThere are two ways of getting data into Spark, through Rows and Encoders. Row objects are the most general way of \\ngetting data into, and out of, Spark and are available in all languages. Each record in a DataFrame must be of Row type \\nas we can see when we collect the following DataFrames. This row is the internal, optimized format that we reference \\nabove.\\n%scala\\nspark.range(2).toDF().collect()\\n%python\\nspark.range(2).collect()\\nSpark Types\\nWe mentioned above Spark has a large number of internal type representations. We include a handy reference table \\non the next several pages in order for you to most easily reference what type, in your specific language, lines up with \\nthe type in Spark.\\nBefore getting to those tables, let’s talk about how we instantiate or declare a column to be of a certain type.\\nTo work with the correct Scala types:\\nimport org.apache.spark.sql.types._\\nval b = ByteType()\\nTo work with the correct Java types you should use the factory methods in the following package:\\nimport org.apache.spark.sql.types.DataTypes;\\nByteType x = DataTypes.ByteType();\\nPython types at time have certain requirements (like the listed requirement for ByteType below.To work with the \\n47\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 46, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='correct Python types:\\nfrom pyspark.sql.types import *\\nb = byteType()\\nScala Type Reference\\nData type\\nValue type in Scala\\nAPI to access or create a data type\\nByteType\\nShortType\\nIntegerType\\nLongType\\nFloatType\\nDoubleType\\nDecimalType\\nStringType\\nBinaryType\\nBooleanType\\nTimestampType\\nDateType\\nArrayType\\nMapType\\nStructType\\nStruct Field\\nByte\\nShort\\nType\\nLong\\nFloat\\nDouble\\njava.math.BigDecimal\\nString\\nArray [Byte]\\nBoolean\\njava.sql.Timestamp\\njava.sql.Date\\nscala.collection.Seq\\nscala.collection.Map\\norg.apache.spark.sql.Row\\nThe value type in Scala of the data \\ntype of this field (For example, Int \\nfor a StructField with the data type \\nIntegerType) \\nByteType\\nShortType\\nIntegerType\\nLongType\\nFloatType\\nDoubleType\\nDecimalType\\nStringType\\nBinaryType\\nBooleanType\\nTimestampType\\nDateType\\nArrayType(elementType, [containsNull]) Note: \\nThe default value of containsNull is true.\\nMapType (keyType, valueType, \\n[valuecontainsNull]) Note: The default value \\nof valuecontainsNull is true.\\nStructType (fields) Note: fields is a Seq of \\nStructFields. Also, two fields with the same \\nname are not allowed. \\nStructField(name, dataType, [nullable]) Note: \\nThe default value of nullable is true.\\n48\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 47, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Java Type Reference\\nData type\\nValue type in Java\\nAPI to access or create a data type\\nByteType\\nShortType\\nIntegerType\\nLongType\\nFloatType\\nDoubleType\\nDecimalType\\nStringType\\nBinaryType\\nBooleanType\\nTimestampType\\nDateType\\nArrayType\\nMapType\\nStructType\\nStruct Field\\nbyte or Byte\\nshort or Short\\ninteger or Integer\\nlong or Long\\nfloat or Float\\ndouble or Double\\njava.math.BigDecimal\\nString\\nbyte []\\nboolean or Boolean\\njava.sql.Timestamp\\njava.sql.Date\\njava.util.List\\njava.util.Map\\norg.apache.spark.sql.Row\\nField value type in Java of the data \\ntype of this field (For example, int \\nfor a StructField with the data type \\nIntegerType)\\nDataTypes.ByteType\\nDataTypes.ShortType\\nDataTypes.IntegerType\\nDataTypes.LongType\\nDataTypes.FloatType\\nDataTypes.DoubleType\\nDataTypes.DecimalType\\nDataTypes.StringType\\nDataTypes.BinaryType\\nDataTypes.BooleanType\\nDataTypes.TimestampType\\nDataTypes.DateType\\nDataTypes.createArrayType(elementType) \\nNote: The value of containsNull will be true \\nDataTypes.createArrayType(elementType, \\ncontainsNull).\\nDataTypes.createMapType(keyType, \\nvalueType) Note: The value of \\nvalueContainsNull will be true. DataTypes.\\ncreateMapType(keyType, valueType, \\nvalueContainsNull)\\nDataTypes.createStructType(fields) Note: \\nfields is a List or an array of StructFields. \\nAlso, two fields with the same name are not \\nallowed.\\nDataTypes.createStructField(name, dataType, \\nnullable)\\n49\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 48, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Python Type Reference\\nData type\\nValue type in Python\\nAPI to access or create a data type\\nByteType\\nShortType\\nIntegerType\\nLongType\\nFloatType\\nDoubleType\\nDecimalType\\nStringType\\nBinaryType\\nBooleanType\\nTimestampType\\nDateType\\nArrayType\\nMapType\\nStructType\\nintor long Note: Numbers will be \\nconverted to 1-byte signed integer \\nnumbers at runtime. Please make sure \\nthat numbers are within the range of \\n-128 to 127.\\nintor long Note: Numbers will be \\nconverted to 2-byte signed integer \\nnumbers at runtime. Please make sure \\nthat numbers are within the range of \\n-32768 to 32767.\\ntype long\\nlong Note: Numbers will be \\nconverted to 8-byte signed integer \\nnumbers at runtime. Please make \\nsure that numbers are within the \\nrange of -9223372036854775808 to \\n9223372036854775807. Otherwise, \\nplease convert data to decimal.Decimal \\nand use DecimalType.\\nfloat Note: Numbers will be converted \\nto 4-byte single-precision floating point \\nnumbers at runtime.\\nfloat\\ndecimal.Decimal\\nstring\\nbytearray\\nbool\\ndatetime.datetime\\ndatetime.date \\nlist, tuple, or array\\ndict\\nField value type in Java of the data \\ntype of this field (For example, int \\nfor a StructField with the data type \\nIntegerType)\\nByteType()\\nShortType()\\nIntegerType()\\nLongType()\\nFloatType()\\nDoubleType()\\nDecimalType()\\nStringType()\\nBinaryType()\\nBooleanType()\\nTimestampType()\\nDateType()\\nArrayType(elementType, [containsNull]) Note: \\nThe default value of containsNull is True.\\nMapType(keyType, valueType, \\n[valueContainsNull]) Note: The default value \\nof valueContainsNull is True.\\nStructType(fields) Note: fields is a Seq of \\nStructFields. Also, two fields with the same \\nname are not allowed.\\n50\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 49, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='SQL\\nDatasets\\nDataFrames\\nCatalyst optimizer\\nPhysical plan\\n Python Type Reference (cont.)\\nData type\\nValue type in Python\\nAPI to access or create a data type\\nStruct Field\\nThe value type in Python of the data \\ntype of this field (For example, Int \\nfor a StructField with the data type \\nIntegerType)\\nStructField(name, dataType, [nullable]) Note: \\nThe default value of nullable is True.\\nIt’s worth keeping in mind that the types may change over time and there may be truncation (especially when  \\ndealing with certain languages lack of precise data types). It’s always worth checking the documentation for the most \\nup to date information.\\nOverview of Structured API Execution\\nIn order to help you understand (and potentially debug) the process of writing and executing code on clusters, let’s \\nwalk through the execution of a single structured API query from user code to executed code. As an overview the  \\nsteps are:\\n1. Write DataFrame/Dataset/SQL Code\\n2. If valid code, Spark converts this to a Logical Plan\\n3. Spark transforms this Logical Plan to a Physical Plan\\n4. Spark then executes this Physical Plan on the cluster\\nTo execute code, we have to write code. This code is then submitted to Spark either through the console or via a \\nsubmitted job. This code then passes through the Catalyst Optimizer which decides how the code should be executed \\nand lays out a plan for doing so, before finally the code is run and the result is returned to the user. \\nFigure \\n51\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 50, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Logical Planning\\nThe first phase of execution is meant to take user code and convert it into a logical plan. This process in illustrated  \\nin the next figure.\\nThis logical plan only represents a set of abstract transformations that do not refer to executors or drivers, it’s purely \\nto convert the user’s set of expressions into the most optimized version. It does this by converting user code into an \\nunresolved logical plan. This unresolved because while your code may be valid, the tables or columns that it refers to \\nmay or may not exist. Spark uses the catalog, a repository of all table and DataFrame information, in order to resolve \\ncolumns and tables in the analyzer. The analyzer may reject the unresolved logical plan if it the required table or \\ncolumn name does not exist in the catalog. If it can resolve it, this result is passed through the optimizer, a collection \\nof rules, which attempts to optimize the logical plan by pushing down predicates or selections.\\nPhysical Planning\\nAfter successfully creating an optimized logical plan, Spark then begins the physical planning process. The physical \\nplan, often called a Spark plan, specifies how the logical plan will execute on the cluster by generating different \\nphysical execution strategies and comparing them through a cost model. An example of the cost comparison might be \\nchoosing how to perform a given join by looking at the physical attributes of a given table (how big the table is or how \\nbig its partitions are.)\\nPhysical planning results in a series of RDDs and transformations. This result is why you may have heard Spark \\nreferred to as a compiler, it takes queries in DataFrames, Datasets, and SQL and compiles them into RDD \\nFigure 2:\\nCatalog\\nUnresolved \\nlogical plan\\nAnalyzer\\nResolved \\nlogical plan\\nOptimizer\\nOptimized \\nlogical plan\\nUser Code\\n52\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 51, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='transformations for you. \\nExecution\\nUpon selecting a physical plan, Spark runs all of this code over RDDs, the lower-level programming interface of Spark \\ncovered in Part III. Spark performs further optimizations by, at runtime, generating native Java Bytecode that can \\nremove whole tasks or stages during execution. Finally the result is returned to the user.\\nFigure \\nOptimized \\nlogical plan\\nPhysical plans\\nBest physical plan\\nExecuted on cluster\\nCost model \\n53\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 52, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Basic Structured Operations\\nChapter Overview\\nIn the previous chapter we introduced the core abstractions of the Structured API. This chapter will move away from \\nthe architectural concepts and towards the tactical tools you will use to manipulate DataFrames and the data within \\nthem. This chapter will focus exclusively on single DataFrame operations and avoid aggregations, window functions, \\nand joins which will all be discussed in depth later in this section.\\nDefinitionally, a DataFrame consists of a series of records (like rows in a table), that are of type Row, and a number \\nof columns (like columns in a spreadsheet) that represent an computation expression that can performed on each \\nindividual record in the dataset. The schema defines the name as well as the type of data in each column. The \\npartitioning of the DataFrame defines the layout of the DataFrame or Dataset’s physical distribution across the cluster. \\nThe partitioning scheme defines how that is broken up, this can be set to be based on values in a certain column or \\nnon-deterministically.\\nLet’s define a DataFrame to work with.\\n%scala\\nval df = spark.read.format(“json”)\\n.load(“/mnt/defg/flight-data/json/2015-summary.json”)\\n%python\\ndf = spark.read.format(“json”)\\\\\\n.load(“/mnt/defg/flight-data/json/2015-summary.json”)\\nWe discussed that a DataFame will have columns, and we use a “schema” to view all of those. We can run the \\nfollowing command in Scala or in Python.\\ndf.printSchema()\\nThe schema ties the logical pieces together and is the starting point to better understand DataFrames. \\n54\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 53, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Schemas\\nA schema defines the column names and types of a DataFrame. We can either let a data source define the schema \\n(called schema on read) or we can define it explicitly ourselves.\\nNOTE \\nDeciding whether or not you need to define a schema prior to reading in your data depends your use case. Often \\ntimes for ad hoc analysis, schema on read works just fine (although at times it can be a bit slow with plain text \\nfile formats like csv or json). However, this can also lead to precision issues like a long type incorrectly set as an \\ninteger when reading in a file. When using Spark for production ETL, it is often a good idea to define your schemas \\nmanually, especially when working with untyped data sources like csv and json because schema inference can \\nvary depending on the type of data that you read in.\\nLet’s start with a simple file we saw in the previous chapter and let the semistructured nature of line-delimited JSON \\ndefine the structure. This data is flight data from the United States Bureau of Transportation statistics.\\n%scala\\nspark.read.format(“json”)\\n.load(“/mnt/defg/flight-data/json/2015-summary.json”)\\n.schema\\nScala will return:\\norg.apache.spark.sql.types.StructType = ...\\nStructType(StructField(DEST_COUNTRY_NAME,StringType,true),\\nStructField(ORIGIN_COUNTRY_NAME,StringType,true),\\nStructField(count,LongType,true))\\n%python\\nspark.read.format(“json”)\\\\\\n.load(“/mnt/defg/flight-data/json/2015-summary.json”)\\\\\\n.schema\\n55\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 54, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Python will return:\\nStructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),\\nStructField(ORIGIN_COUNTRY_NAME,StringType,true),\\nStructField(count,LongType,true)))\\nA schema is a StructType made up of a number of fields, StructFields, that have a name, type, and a boolean \\nflag which specifies whether or not that column can contain missing or null values. Schemas can also contain other \\nStructType (Spark’s complex types). We will see this in the next chapter when we discuss working with complex \\ntypes.\\nHere’s how to create, and enforce a specific schema on a DataFrame. If the types in the data (at runtime), do not \\nmatch the schema. Spark will throw an error.\\n%scala\\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\\nval myManualSchema = new StructType(Array(\\nnew StructField(“DEST_COUNTRY_NAME”, StringType, true),\\nnew StructField(“ORIGIN_COUNTRY_NAME”, StringType, true),\\nnew StructField(“count”, LongType, false) // just to illustrate flipping  \\nthis flag\\n))\\nval df = spark.read.format(“json”)\\n.schema(myManualSchema)\\n.load(“/mnt/defg/flight-data/json/2015-summary.json”)\\nHere’s how to do the same in Python.\\n%python\\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\\n56\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 55, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='myManualSchema = StructType([\\nStructField(“DEST_COUNTRY_NAME”, StringType(), True),\\nStructField(“ORIGIN_COUNTRY_NAME”, StringType(), True),\\nStructField(“count”, LongType(), False)\\n])\\ndf = spark.read.format(“json”)\\\\\\n.schema(myManualSchema)\\\\\\n.load(“/mnt/defg/flight-data/json/2015-summary.json”)\\nAs discussed in the previous chapter, we cannot simply set types via the per language types because Spark maintains \\nits own type information. Let’s now discuss what schemas define, columns.\\nColumns and Expressions\\nTo users, columns in Spark are similar to columns in a spreadsheet, R dataframe, pandas DataFrame. We can select, \\nmanipulate, and remove columns from DataFrames and these operations are represented as expressions.\\nTo Spark, columns are logical constructions that simply represent a value computed on a per-record basis by means \\nof an expression. This means, in order to have a real value for a column, we need to have a row, and in order to \\nhave a row we need to have a DataFrame. This means that we cannot manipulate an actual column outside of a \\nDataFrame, we can only manipulate a logical column’s expressions then perform that expression within the context of \\na DataFrame.\\nColumns\\nThere are a lot of different ways to construct and or refer to columns but the two simplest ways are with the col or \\ncolumn functions. To use either of these functions, we pass in a column name.\\n%scala\\nimport org.apache.spark.sql.functions.{col, column}\\ncol(“someColumnName”)\\ncolumn(“someColumnName”)\\n%python\\nfrom pyspark.sql.functions import col, column\\ncol(“someColumnName”)\\n57\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 56, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='column(“someColumnName”)\\nWe will stick to using col throughout this book. As mentioned, this column may or may not exist in our of our \\nDataFrames. This is because, as we saw in the previous chapter, columns are not resolved until we compare the \\ncolumn names with those we are maintaining in the catalog. Column and table resolution happens in the analyzer \\nphase as discussed in the first chapter in this section.\\nNOTE \\nAbove we mentioned two different ways of referring to columns. Scala has some unique language features that \\nallow for more shorthand ways of referring to columns. These bits of syntactic sugar perform the exact same thing \\nas what we have already, namely creating a column, and provide no performance improvement.\\n%scala\\n$”myColumn”\\n‘myColumn\\nThe $ allows us to designate a string as a special string that should refer to an expression. The tick mark ‘ is a special \\nthing called a symbol, that is Scalaspecific construct of referring to some identifier. They both perform the same thing \\nand are shorthand ways of referring to columns by name. You’ll likely see all the above references when you read \\ndifferent people’s spark code. We leave it to the reader for you to use whatever is most comfortable and maintainable \\nfor you.\\nExplicit Column References\\nIf you need to refer to a specific DataFrame’s column, you can use the col method on the specific DataFrame. This \\ncan be useful when you are performing a join and need to refer to a specific column in one DataFrame that may share \\na name with another column in the joined DataFrame. We will see this in the joins chapter. As an added benefit, Spark \\ndoes not need to resolve this column itself (during the analyzer phase) because we did that for Spark.\\ndf.col(“count”)\\nExpressions\\nNow we mentioned that columns are expressions, so what is an expression? An expression is a set of transformations \\n58\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 57, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='on one or more values in a record in a DataFrame. Think of it like a function that takes as input one or more column \\nnames, resolves them and then potentially applies more expressions to create a single value for each record in the \\ndataset. Importantly, this “single value” can actually be a complex type like a Map type or Array type.\\nIn the simplest case, an expression, created via the expr function, is just a DataFrame column reference.\\nimport org.apache.spark.sql.functions.{expr, col}\\nIn this simple instance, expr(“someCol”) is equivalent to col(“someCol”).\\nColumns as Expressions\\nColumns provide a subset of expression functionality. If you use col() and wish to perform transformations on \\nthat column, you must perform those on that column reference. When using an expression, the expr function can \\nactually parse transformations and column references from a string and can subsequently be passed into further \\ntransformations. Let’s look at some examples. \\nexpr(“someCol - 5”) is the same transformation as performing col(“someCol”) - 5 or even \\nexpr(“someCol”) - 5. That’s because Spark compiles these to a logical tree specifying the order of operations. \\nThis might be a bit confusing at first, but remember a couple of key points.\\n1. Columns are just expressions.\\n2. Columns and transformations of those column compile to the same logical plan as parsed expressions.\\nLet’s ground this with an example.\\n(((col(“someCol”) + 5) * 200) - 6) < col(“otherCol”) \\n59\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 58, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Figure 1:\\nFigure 1 shows an illustration of that logical tree.\\nThis might look familiar because it’s a directed acyclic graph. This graph is represented equivalently with the  \\nfollowing code.\\n%scala\\nimport org.apache.spark.sql.functions.expr\\nexpr(“(((someCol + 5) * 200) - 6) < otherCol”)\\n%python\\nfrom pyspark.sql.functions import expr\\nexpr(“(((someCol + 5) * 200) - 6) < otherCol”)\\nThis is an extremely important point to reinforce. Notice how the previous expression is actually valid SQL code \\nas well, just like you might put in a SELECT statement? That’s because this SQL expression and the previous \\nDataFrame code compile to the same underlying logical tree prior to execution. This means you can write your \\nexpressions as DataFrame code or as SQL expressions and get the exact same benefits. You likely saw all of this in the \\nfirst chapters of the book and we covered this more extensively in the Overview of the Structured APIs chapter.\\nSome col\\n+\\nOther col\\n6\\n200\\n5\\n*\\n_\\n<\\n60\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 59, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Accessing a DataFrame’s Columns\\nSometimes you’ll need to see a DataFrame’s columns, you can do this by doing something like printSchema however \\nif you want to programmatically access columns, you can use the columns method to see all columns listed.\\nspark.read.format(“json”)\\n.load(“/mnt/defg/flight-data/json/2015-summary.json”)\\n.columns\\nRecords and Rows\\nIn Spark, a record or row makes up a “row” in a DataFrame. A logical record or row is an object of type Row. Row \\nobjects are the objects that column expressions operate on to produce some usable value. Row objects represent \\nphysical byte arrays. The byte array interface is never shown to users because we only use column expressions to \\nmanipulate them.\\nYou’ll notice collections that return values will always return one or more Row types.\\nNOTE\\nwe will use lowercase “row” and “record” interchangeably in this chapter, with a focus on the latter. A capitalized \\n“Row” will refer to the Row object. We can see a row by calling first on our DataFrame.\\n%scala\\ndf.first()\\n%python\\ndf.first()\\nCreating Rows\\nYou can create rows by manually instantiating a Row object with the values that below in each column. It’s important \\nto note that only DataFrames have schema. Rows themselves do not have schemas. This means if you create a Row \\n61\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 60, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='manually, you must specify the values in the same order as the schema of the DataFrame they may be appended to. \\nWe will see this when we discuss creating DataFrames.\\n%scala\\nimport org.apache.spark.sql.Row\\nval myRow = Row(“Hello”, null, 1, false)\\n%python\\nfrom pyspark.sql import Row\\nmyRow = Row(“Hello”, None, 1, False)\\nAccessing data in rows is equally as easy. We just specify the position. However because Spark maintains its own type \\ninformation, we will have to manually coerce this to the correct type in our respective language.\\nFor example in Scala, we have to either use the helper methods or explicitly coerce the values.\\n%scala\\nmyRow(0) // type Any\\nmyRow(0).asInstanceOf[String] // String\\nmyRow.getString(0) // String\\nmyRow.getInt(2) // String\\nThere exist one of these helper functions for each corresponding Spark and Scala type. In Python, we do not have to \\nworry about this, Spark will automatically return the correct type by location in the Row Object.\\n%python\\nmyRow[0]\\nmyRow[2]\\n62\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 61, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='You can also explicitly return a set of Data in the corresponding JVM objects by leverage the Dataset APIs. This is \\ncovered at the end of the Structured API section.\\nDataFrame Transformations\\nNow that we briefly defined the core parts of a DataFrame, we will move onto manipulating DataFrames. When \\nworking with individual DataFrames there are some fundamental objectives. These break down into several core \\noperations.\\n• We can add rows or columns\\n• We can remove rows or columns\\n• We can transform a row into a column (or vice versa)\\n• We can change the order of rows based on the values in columns\\nLuckily we can translate all of these into simple transformations, the most common being those that take one column, \\nchange it row by row, and then return our results. \\nFigure \\nCreating DataFrames\\nAs we saw previously, we can create DataFrames from raw data sources. This is covered extensively in the Data \\nSources chapter however we will use them now to create an example DataFrame. For illustration purposes later  \\nin this chapter, we will also register this as a temporary view so that we can query it with SQL.\\nRemove columns or rows\\nRow into a column \\nor column into a row\\nAdd rows or columns\\nSort data by values in rows\\n63\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 62, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nval df = spark.read.format(“json”)\\n.load(“/mnt/defg/flight-data/json/2015-summary.json”)\\ndf.createOrReplaceTempView(“dfTable”)\\n%python\\ndf = spark.read.format(“json”)\\\\\\n.load(“/mnt/defg/flight-data/json/2015-summary.json”)\\ndf.createOrReplaceTempView(“dfTable”)\\nWe can also create DataFrames on the fly by taking a set of rows and converting them to a DataFrame.\\n%scala\\nimport org.apache.spark.sql.Row\\nimport org.apache.spark.sql.types.{StructField, StructType,\\n \\n \\n \\n \\n \\n \\nStringType, LongType}\\nval myManualSchema = new StructType(Array(\\nnew StructField(“some”, StringType, true),\\nnew StructField(“col”, StringType, true),\\nnew StructField(“names”, LongType, false) // just to illustrate flipping  \\nthis flag\\n))\\nval myRows = Seq(Row(“Hello”, null, 1L))\\nval myRDD = spark.sparkContext.parallelize(myRows)\\nval myDf = spark.createDataFrame(myRDD, myManualSchema)\\nmyDf.show()\\nNOTE: In Scala we can also take advantage of Spark’s implicits in the console (and if you import them in your jar  \\ncode), by running to DF on a Seq type. This does not play well with null types, so it’s not necessarily recommended for \\nproduction use cases.\\n64\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 63, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nval myDF = Seq((“Hello”, 2, 1L)).toDF()\\n%python\\nfrom pyspark.sql import Row\\nfrom pyspark.sql.types import StructField, StructType,\\\\\\n \\n \\n \\n \\n \\nStringType, LongType\\nmyManualSchema = StructType([\\nStructField(“some”, StringType(), True),\\nStructField(“col”, StringType(), True),\\nStructField(“names”, LongType(), False)\\n])\\nmyRow = Row(“Hello”, None, 1)\\nmyDf = spark.createDataFrame([myRow], myManualSchema)\\nmyDf.show()\\nNow that we know how to create DataFrames, let’s go over their most useful methods that you’re going to be using \\nare: the select method when you’re working with columns or expressions and the selectExpr method when \\nyou’re working with expressions in strings. Naturally some transformations are not specified as a methods on \\ncolumns, therefore there exists a group of functions found in the org.apache.spark.sql.functions package. \\nWith these three tools, you should be able to solve the vast majority of transformation challenges that you may \\nencourage in DataFrames.\\nSelect & SelectExpr\\nSelect and SelectExpr allow us to do the DataFrame equivalent of SQL queries on a table of data.\\nSELECT * FROM dataFrameTable\\nSELECT columnName FROM dataFrameTable\\nSELECT columnName * 10, otherColumn, someOtherCol as c FROM dataFrameTable\\n65\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 64, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='In the simplest possible terms, it allows us to manipulate columns in our DataFrames. Let’s walk through some \\nexamples on DataFrames to talk about some of the different ways of approaching this problem. The easiest way is just \\nto use the select method and pass in the column names as string that you would like to work with.\\n%scala\\ndf.select(“DEST_COUNTRY_NAME”).show(2)\\n%python\\ndf.select(“DEST_COUNTRY_NAME”).show(2)\\n%sql\\nSELECT DEST_COUNTRY_NAME\\nFROM dfTable\\nLIMIT 2\\nYou can select multiple columns using the same style of query, just add more column name strings to our select \\nmethod call.\\n%scala\\ndf.select(\\n“DEST_COUNTRY_NAME”,\\n“ORIGIN_COUNTRY_NAME”)\\n.show(2)\\n%python\\ndf.select(\\n“DEST_COUNTRY_NAME”,\\n“ORIGIN_COUNTRY_NAME” )\\\\\\n.show(2)\\n%sql\\n66\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 65, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='SELECT\\nDEST_COUNTRY_NAME, \\nORIGIN_COUNTRY_NAME\\nFROM\\ndfTable\\nLIMIT 2\\nAs covered in Columns and Expressions, we can refer to columns in a number of different ways; as a user all you need \\nto keep in mind is that we can use them interchangeably.\\n%scala\\nimport org.apache.spark.sql.functions.{expr, col, column}\\ndf.select(\\ndf.col(“DEST_COUNTRY_NAME”),\\ncol(“DEST_COUNTRY_NAME”),\\ncolumn(“DEST_COUNTRY_NAME”),\\n‘DEST_COUNTRY_NAME,\\n$”DEST_COUNTRY_NAME”,\\nexpr(“DEST_COUNTRY_NAME”)\\n).show(2)\\n%python\\nfrom pyspark.sql.functions import expr, col, column\\ndf.select(\\nexpr(“DEST_COUNTRY_NAME”),\\ncol(“DEST_COUNTRY_NAME”),\\ncolumn(“DEST_COUNTRY_NAME”))\\\\\\n.show(2)\\nOne common error is attempting to mix Column objects and strings. For example,the below code will result in a  \\ncompiler error.\\ndf.select(col(“DEST_COUNTRY_NAME”), “DEST_COUNTRY_NAME”)\\n67\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 66, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='As we’ve seen thus far, expr is the most flexible reference that we can use. It can refer to a plain column or a string \\nmanipulation of a column. To illustrate, let’s change our column name, then change it back as an example using the \\nAS keyword and then the alias method on the column.\\n%scala\\ndf.select(expr(“DEST_COUNTRY_NAME AS destination”))\\n%python\\ndf.select(expr(“DEST_COUNTRY_NAME AS destination”))\\n%sql\\nSELECT\\nDEST_COUNTRY_NAME as destination\\nFROM\\ndfTable\\nWe can further manipulate the result of our expression as another expression.\\n%scala\\ndf.select(\\nexpr(“DEST_COUNTRY_NAME as destination”).alias(“DEST_COUNTRY_NAME”)\\n)\\n%python\\ndf.select(\\nexpr(“DEST_COUNTRY_NAME as destination”).alias(“DEST_COUNTRY_NAME”)\\n)\\nBecause select followed by a series of expr is such a common pattern, Spark has a shorthand for doing so \\nefficiently: selectExpr. This is probably the most convenient interface for everyday use.\\n68\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 67, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\ndf.selectExpr(\\n“DEST_COUNTRY_NAME as newColumnName”,\\n“DEST_COUNTRY_NAME”\\n).show(2)\\n%python\\ndf.selectExpr(\\n“DEST_COUNTRY_NAME as newColumnName”,\\n“DEST_COUNTRY_NAME”\\n).show(2)\\nThis opens up the true power of Spark. We can treat selectExpr as a simple way to build up complex expressions \\nthat create new DataFrames. In fact, we can add any valid non-aggregating SQL statement and as long as the columns \\nresolve — it will be valid! Here’s a simple example that adds a new column withinCountry to our DataFrame that \\nspecifies whether or not the destination and origin are the same.\\n%scala\\ndf.selectExpr(\\n“*”, // all original columns\\n“(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry”\\n).show(2)\\n%python\\ndf.selectExpr(\\n“*”, # all original columns\\n“(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry” )\\\\\\n.show(2)\\n%sql\\nSELECT\\n*,\\n(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\\nFROM\\ndfTable\\n69\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 68, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Now we’ve learning about select and select expression. With these we can specify aggregations over the entire \\nDataFrame by leveraging the functions that we have. These look just like what we have been showing so far.\\n%scala\\ndf.selectExpr(“avg(count)”, “count(distinct(DEST_COUNTRY_NAME))”).show(2)\\n%python\\ndf.selectExpr(“avg(count)”, “count(distinct(DEST_COUNTRY_NAME))”).show(2)\\n%sql\\nSELECT\\navg(count),\\ncount(distinct(DEST_COUNTRY_NAME))\\nFROM\\ndfTable\\nConverting to Spark Types (Literals)\\nSometimes we need to pass explicit values into Spark that aren’t a new column but are just a value. This might \\nbe a constant value or something we’ll need to compare to later on. The way we do this is through literals. This is \\nbasically a translation from a given programming language’s literal value to one that Spark understands. Literals are \\nexpressions and can be used in the same way. \\n%scala\\nimport org.apache.spark.sql.functions.lit\\ndf.select(\\nexpr(“*”),\\nlit(1).as(“something”)\\n).show(2)\\n%python\\nfrom pyspark.sql.functions import lit\\n70\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 69, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='df.select(\\nexpr(“*”),\\nlit(1).alias(“One”)\\n).show(2)\\nIn SQL, literals are just the specific value.\\n%sql\\nSELECT\\n*,\\n1 as One\\nFROM\\ndfTable\\nLIMIT 2\\nThis will come up when you might need to check if a date is greater than some constant or some value.\\nAdding Columns\\nThere’s also a more formal way of adding a new column to a DataFrame using the withColumn method on our \\nDataFrame. For example, let’s add a column that just adds the number one as a column.\\n%scala\\ndf.withColumn(“numberOne”, lit(1)).show(2)\\n%python\\ndf.withColumn(“numberOne”, lit(1)).show(2)\\n%sql\\nSELECT\\n1 as numberOne\\nFROM\\ndfTable\\nLIMIT 2\\n71\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 70, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Let’s do something a bit more interesting and make it an actual expression. Let’s set a boolean flag for when the origin \\ncountry is the same as the destination country.\\n%scala\\ndf.withColumn(\\n“withinCountry”,\\nexpr(“ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME”)\\n).show(2)\\n%python\\ndf.withColumn(\\n“withinCountry”,\\nexpr(“ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME”))\\\\\\n.show(2)\\nYou should notice that the withColumn function takes two arguments: the column name and the expression that \\nwill create the value for that given row in the DataFrame. Interestingly, we can also rename a column this way.\\n%scala\\ndf.withColumn(\\n“Destination”,\\ndf.col(“DEST_COUNTRY_NAME”))\\n.columns\\nRenaming Columns\\nAlthough we can rename a column in the above manner, it’s often much easier (and readable) to use the \\nwithColumnRenamed method. This will rename the column with the name of the string in the first argument, to the \\nstring in the second argument.\\n72\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 71, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\ndf.withColumnRenamed(“DEST_COUNTRY_NAME”, “dest”).columns\\n%python\\ndf.withColumnRenamed(“DEST_COUNTRY_NAME”, “dest”).columns\\nReserved Characters and Keywords in Column Names\\nOne thing that you may come across is reserved characters like spaces or dashes in column names. Handling \\nthese means escaping column names appropriately. In Spark this is done with backtick (‘) characters. Let’s use the \\nwithColumn that we just learned about to create a Column with reserved characters.\\n%scala\\nimport org.apache.spark.sql.functions.expr\\nval dfWithLongColName = df\\n.withColumn(\\n“This Long Column-Name”,\\nexpr(“ORIGIN_COUNTRY_NAME”))\\n%python\\ndfWithLongColName = df\\\\\\n.withColumn(\\n“This Long Column-Name”,\\nexpr(“ORIGIN_COUNTRY_NAME”))\\nWe did not have to escape the column above because the first argument to withColumn is just a string for the new \\ncolumn name. We only have to use backticks when referencing a column in an expression.\\n73\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 72, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\ndfWithLongColName\\n.selectExpr(\\n“`This Long Column-Name`”,\\n“`This Long Column-Name` as `new col`”)\\n.show(2)\\n%python\\ndfWithLongColName\\\\\\n.selectExpr(\\n“`This Long Column-Name`”,\\n“`This Long Column-Name` as `new col`” )\\\\\\n.show(2)\\ndfWithLongColName.createOrReplaceTempView(“dfTableLong”)\\n%sql\\nSELECT `This Long Column-Name` FROM dfTableLong\\nWe can refer to columns with reserved characters (and not escape them) if doing an explicit string to column \\nreference, which gets interpreted as a literal instead of an expression. We only have to escape expressions that \\nleverage reserved characters or keywords. The following two examples both result in the same DataFrame.\\n%scala\\ndfWithLongColName.select(col(“This Long Column-Name”)).columns\\n%python\\ndfWithLongColName.select(expr(“`This Long Column-Name`”)).columns\\nRemoving Columns\\nNow that we’ve created this column, let’s take a look at how we can remove columns from DataFrames. You likely \\nalready noticed that we can do this with select. However there is also a dedicated method called drop.\\n74\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 73, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='df.drop(“ORIGIN_COUNTRY_NAME”).columns\\nWe can drop multiple columns by passing in multiple columns as arguments.\\ndfWithLongColName.drop(“ORIGIN_COUNTRY_NAME”, “DEST_COUNTRY_NAME”)\\nChanging a Column’s Type (cast)\\nSometimes we may need to convert from one type to another, for example if we have a set of StringType that \\nshould be integers. We can convert columns from one type to another by casting the column from one type to another. \\nFor instance let’s convert our count column from an integer to a Long type.\\ndf.printSchema()\\ndf.withColumn(“count”, col(“count”).cast(“int”)).printSchema()\\n%sql\\nSELECT\\ncast(count as int)\\nFROM\\ndfTable\\nFiltering Rows\\nTo filter rows we create an expression that evaluates to true or false. We then filter out the rows that have expression \\nthat is equal to false. The most common way to do this with DataFrames is to create either an expression as a String \\nor build an expression with a set of column manipulations. There are two methods to perform this operation, we can \\nuse where or filter and they both will perform the same operation and accept the same argument types when \\nused with DataFrames. The Dataset API has slightly different options and please refer to the Dataset chapter for more \\ninformation.\\nThe following filters are equivalent.\\n75\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 74, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nval colCondition = df.filter(col(“count”) < 2).take(2)\\nval conditional = df.where(“count < 2”).take(2)\\n%python\\ncolCondition = df.filter(col(“count”) < 2).take(2)\\nconditional = df.where(“count < 2”).take(2)\\n%sql\\nSELECT\\n*\\nFROM dfTable\\nWHERE\\ncount < 2\\nInstinctually you may want to put multiple filters into the same expression. While this is possible, it is not always \\nuseful because Spark automatically performs all filtering operations at the same time. This is called pipelining and \\nhelps make Spark very efficient. As a user, that means if you want to specify multiple AND filters, just chain them \\nsequentially and let Spark handle the rest.\\n%scala\\ndf.where(col(“count”) < 2)\\n.where(col(“ORIGIN_COUNTRY_NAME”) =!= “Croatia”)\\n.show(2)\\n%python\\ndf.where(col(“count”) < 2)\\\\\\n.where(col(“ORIGIN_COUNTRY_NAME”) != “Croatia”)\\\\\\n.show(2)\\n%sql\\nSELECT\\n*\\nFROM dfTable\\n76\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 75, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='WHERE\\ncount < 2 AND\\nORIGIN_COUNTRY_NAME != “Croatia”\\nGetting Unique Rows\\nA very common use case is to get the unique or distinct values in a DataFrame. These values can be in one or more \\ncolumns. The way we do this is with the distinct method on a DataFrame that will allow us to deduplicate \\nany rows that are in that DataFrame. For instance let’s get the unique origins in our dataset. This of course is a \\ntransformation that will return a new DataFrame with only unique rows.\\n%scala\\ndf.select(“ORIGIN_COUNTRY_NAME”, “DEST_COUNTRY_NAME”).count()\\n%python\\ndf.select(“ORIGIN_COUNTRY_NAME”, “DEST_COUNTRY_NAME”).count()\\n%sql\\nSELECT\\nCOUNT(DISTINCT ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)\\nFROM dfTable\\n%scala\\ndf.select(“ORIGIN_COUNTRY_NAME”).distinct().count()\\n%python\\ndf.select(“ORIGIN_COUNTRY_NAME”).distinct().count()\\n%sql\\nSELECT\\nCOUNT(DISTINCT ORIGIN_COUNTRY_NAME)\\nFROM dfTable\\n77\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 76, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Random Samples\\nSometimes you may just want to sample some random records from your DataFrame. This is done with the sample \\nmethod on a DataFrame that allows you to specify a fraction of rows to extract from a DataFrame and whether you’d \\nlike to sample with or without replacement.\\nval seed = 5\\nval withReplacement = false\\nval fraction = 0.5\\ndf.sample(withReplacement, fraction, seed).count()\\n%python\\nseed = 5\\nwithReplacement = False\\nfraction = 0.5\\ndf.sample(withReplacement, fraction, seed).count()\\nRandom Splits\\nRandom splits can be helpful when you need to break up your DataFrame, randomly, in such a way that sampling \\nrandom cannot guarantee that all records are in one of the DataFrames that you’re sampling from. This is often \\nused with machine learning algorithms to create training, validation, and test sets. In this example we’ll split our \\nDataFrame into two different DataFrames by setting the weights by which we will split the DataFrame (these are the \\narguments to the function). Since this method involves some randomness, we will also specify a seed. It’s important \\nto note that if you don’t specify a proportion for each DataFrame that adds up to one, they will be normalized so that \\nthey do.\\n%scala\\nval dataFrames = df.randomSplit(Array(0.25, 0.75), seed)\\ndataFrames(0).count() > dataFrames(1).count()\\n%python\\ndataFrames = df.randomSplit([0.25, 0.75], seed)\\ndataFrames[0].count() > dataFrames[1].count()\\n78\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 77, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Concatenating and Appending Rows to a DataFrame\\nAs we learned in the previous section, DataFrames are immutable. This means users cannot append to DataFrames \\nbecause that would be changing it. In order to append to a DataFrame, you must union the original DataFrame along \\nwith the new DataFrame. This just concatenates the two DataFrames together. To union two DataFrames, you have to \\nbe sure that they have the same schema and number of columns, else the union will fail.\\n%scala\\nimport org.apache.spark.sql.Row\\nval schema = df.schema\\nval newRows = Seq(\\nRow(“New Country”, “Other Country”, 5L),\\nRow(“New Country 2”, “Other Country 3”, 1L)\\n)\\nval parallelizedRows = spark.sparkContext.parallelize(newRows)\\nval newDF = spark.createDataFrame(parallelizedRows, schema)\\ndf.union(newDF)\\n.where(“count = 1”)\\n.where($”ORIGIN_COUNTRY_NAME” =!= “United States”)\\n.show() // get all of them and we’ll see our new rows at the end\\n%python\\nfrom pyspark.sql import Row\\nschema = df.schema\\nnewRows = [\\nRow(“New Country”, “Other Country”, 5L),\\nRow(“New Country 2”, “Other Country 3”, 1L)\\n]\\nparallelizedRows = spark.sparkContext.parallelize(newRows)\\nnewDF = spark.createDataFrame(parallelizedRows, schema)\\n%python\\n79\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 78, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='df.union(newDF)\\\\\\n.where(“count = 1”)\\\\\\n.where(col(“ORIGIN_COUNTRY_NAME”) != “United States”)\\\\\\n.show()\\nAs expected, you’ll have to use this new DataFrame reference in order to refer to the DataFrame with the newly \\nappended rows. A common way to do this is to make the DataFrame into a view or register it as a table so that you can \\nreference it more dynamically in your code.\\nSorting Rows\\nWhen we sort the values in a DataFrame, we always want to sort with either the largest or smallest values at the top of \\na DataFrame. There are two equivalent operations to do this sort and orderBy that work the exact same way. They \\naccept both column expressions and strings as well as multiple columns. The default is to sort in ascending order.\\n%scala\\ndf.sort(“count”).show(5)\\ndf.orderBy(“count”, “DEST_COUNTRY_NAME”).show(5)\\ndf.orderBy(col(“count”), col(“DEST_COUNTRY_NAME”)).show(5)\\n%python\\ndf.sort(“count”).show(5)\\ndf.orderBy(“count”, “DEST_COUNTRY_NAME”).show(5)\\ndf.orderBy(col(“count”), col(“DEST_COUNTRY_NAME”)).show(5)\\nTo more explicit specify sort direction we have to use the asc and desc functions if operating on a column. These allow \\nus to specify the order that a given column should be sorted in.\\n%scala\\nimport org.apache.spark.sql.functions.{desc, asc}\\n80\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 79, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='df.orderBy(expr(“count desc”)).show(2)\\ndf.orderBy(desc(“count”), asc(“DEST_COUNTRY_NAME”)).show(2)\\n%python\\nfrom pyspark.sql.functions import desc, asc\\ndf.orderBy(expr(“count desc”)).show(2)\\ndf.orderBy(desc(col(“count”)), asc(col(“DEST_COUNTRY_NAME”))).show(2)\\n%sql\\nSELECT *\\nFROM dfTable\\nORDER BY count DESC, DEST_COUNTRY_NAME ASC\\nFor optimization purposes, it can sometimes be advisable to sort within each partition before another set of \\ntransformations. We can do this with the sortWithinPartitions method.\\n%scala\\nspark.read.format(“json”)\\n.load(“/mnt/defg/flight-data/json/*-summary.json”)\\n.sortWithinPartitions(“count”)\\n%python\\nspark.read.format(“json”)\\\\\\n.load(“/mnt/defg/flight-data/json/*-summary.json”)\\\\\\n.sortWithinPartitions(“count”)\\nWe will discuss this more when discussing tuning and optimization in Section 3.\\nLimit\\nOften times you may just want the top ten of some DataFrame. For example, you might want to only work with the top \\n50 of some dataset. We do this with the limit method.\\n81\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 80, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\ndf.limit(5).show()\\n%python\\ndf.limit(5).show()\\n%scala\\ndf.orderBy(expr(“count desc”)).limit(6).show()\\n%python\\ndf.orderBy(expr(“count desc”)).limit(6).show()\\n%sql\\nSELECT *\\nFROM dfTable\\nLIMIT 6\\nRepartition and Coalesce\\nAnother important optimization opportunity is to partition the data according to some frequently filtered columns \\nwhich controls the physical layout of data across the cluster including the partitioning scheme and the number of \\npartitions.\\nRepartition will incur a full shuffle of the data, regardless of whether or not one is necessary. This means that you \\nshould typically only repartition when the future number of partitions is greater than your current number of \\npartitions or when you are looking to partition by a set of columns.\\n%scala\\ndf.rdd.getNumPartitions\\n%python\\ndf.rdd.getNumPartitions()\\n%scala\\ndf.repartition(5)\\n82\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 81, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%python\\ndf.repartition(5)\\nIf we know we are going to be filtering by a certain column often, it can be worth repartitioning based on that column.\\n%scala\\ndf.repartition(col(“DEST_COUNTRY_NAME”))\\n%python\\ndf.repartition(col(“DEST_COUNTRY_NAME”))\\nWe can optionally specify the number of partitions we would like too.\\n%scala\\ndf.repartition(5, col(“DEST_COUNTRY_NAME”))\\n%python\\ndf.repartition(5, col(“DEST_COUNTRY_NAME”))\\nCoalesce on the other hand will not incur a full shuffle and will try to combine partitions. This operation will shuffle \\nour data into 5 partitions based on the destination country name, then coalesce them (without a full shuffle).\\n%scala\\ndf.repartition(5, col(“DEST_COUNTRY_NAME”)).coalesce(2)\\n%python\\ndf.repartition(5, col(“DEST_COUNTRY_NAME”)).coalesce(2)\\n83\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 82, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Collecting Rows to the Driver\\nAs we covered in the previous chapters. Spark has a Driver that maintains cluster information and runs user code. This \\nmeans that when we call some method to collect data, this is collected to the Spark Driver. Thus far we did not talk \\nexplicitly about this operation however we used several different methods for doing that that are effectively all the \\nsame. collect gets all data from the entire DataFrame, take selects the first N rows, show prints out a number of \\nrows nicely. See the appendix for collecting data for the complete list.\\n%scala\\nval collectDF = df.limit(10)\\ncollectDF.take(5) // take works with an Integer count\\ncollectDF.show() // this prints it out nicely\\ncollectDF.show(5, false)\\ncollectDF.collect()\\n%python\\ncollectDF = df.limit(10)\\ncollectDF.take(5) # take works with an Integer count\\ncollectDF.show() # this prints it out nicely\\ncollectDF.show(5, False)\\ncollectDF.collect()\\n84\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 83, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Working with Different Types of Data\\nChapter Overview\\nIn the previous chapter, we covered basic DataFrame concepts and abstractions. This chapter will cover building \\nexpressions, which are the bread and butter of Spark’s structured operations. This chapter will cover working with a \\nvariety of different kinds of data including:\\n• Booleans\\n• Numbers\\n• Strings\\n• Dates and Timestamps\\n• Handling Null\\n• Complex Types\\n• User Defined Functions\\nWhere to Look for APIs\\nBefore we get started, it’s worth explaining where you as a user should start looking for transformations. Spark is \\na growing project and any book (including this one) is a snapshot in time. Therefore it is our priority to educate \\nyou as a user as to where you should look for functions in order to transform your data. The key places to look for \\ntransformations are:\\nDataFrame (Dataset) Methods. This is actually a bit of a trick because a DataFrame is just a Dataset of Row types \\nso you’ll actually end up looking at the Dataset methods. These are available at: http://spark.apache.org/docs/latest/\\napi/scala/index.html#org.apache.spark.sql.Dataset\\nDataset sub-modules like DataFrameStatFunctions and DataFrameNaFunctions that have more \\nmethods. These are usually domain specific sets of functions and methods that only make sense in a certain \\ncontext. For example, DataFrameStatFunctions holds a variety of statistically related functions while \\nDataFrameNaFunctions refers to functions that are relevant when working with null data.\\nNull Functions: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.\\nDataFrameStatFunctions\\n85\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 84, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Stat Functions: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.\\nDataFrameNaFunctions\\nColumn Methods. These were introduced for the most part in the previous chapter are hold a variety of general \\ncolumn related methods like alias or contains. These are available at: http://spark.apache.org/docs/latest/api/\\nscala/index html#org.apache.spark.sql.Column\\norg.apache.spark.sql.functions contains a variety of functions for a variety of different data types. Often \\nyou’ll see the entire package imported because they are used so often. These are available at: http://spark.apache.\\norg/docs/ latest/api/scala/index.html#org.apache.spark.sql.functions$\\nNow this may feel a bit overwhelming but have no fear, the majority of these functions are ones that you will find in \\nSQL and analytics systems. All of these tools exist to achieve one purpose, to transform rows of data in one format or \\nstructure to another. This may create more rows or reduce the number of rows available. To get stated, let’s read in \\nthe DataFrame that we’ll be using for this analysis.\\n%scala\\nval df = spark.read.format(“csv”)\\n.option(“header”, “true”)\\n.option(“inferSchema”, “true”)\\n.load(“/mnt/defg/retail-data/by-day/2010-12-01.csv”)\\ndf.printSchema()\\ndf.createOrReplaceTempView(“dfTable”)\\n%python\\ndf = spark.read.format(“csv”)\\\\\\n.option(“header”, “true”)\\\\\\n.option(“inferSchema”, “true”)\\\\\\n.load(“/mnt/defg/retail-data/by-day/2010-12-01.csv”)\\ndf.printSchema()\\ndf.createOrReplaceTempView(“dfTable”)\\nThese will print the schema nicely.\\n \\nroot\\n|-- InvoiceNo: string (nullable = true)\\n86\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 85, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='|-- StockCode: string (nullable = true)\\n|-- Description: string (nullable = true)\\n|-- Quantity: integer (nullable = true)\\n|-- InvoiceDate: timestamp (nullable = true)\\n|-- UnitPrice: double (nullable = true)\\n|-- CustomerID: double (nullable = true)\\n|-- Country: string (nullable = true)\\nWorking with Booleans\\nBooleans are foundational when it comes to data analysis because they are the foundation for all filtering. Boolean \\nstatements consist of four elements: and, or, true and false. We use these simple structures to build logical \\nstatements that evaluate to either true or false. These statements are often used as conditional requirements \\nwhere a row of data must either pass this test (evaluate to true) or else it will be filtered out.\\nLet’s use our retail dataset to explore working with booleans. We can specify equality as well as less or greater than.\\n%scala\\nimport org.apache.spark.sql.functions.col\\ndf.where(col(“InvoiceNo”).equalTo(536365))\\n.select(“InvoiceNo”, “Description”)\\n.show(5, false)\\nNOTE\\nScala has some particular semantics around the use of == and ===. In Spark, if you wish to filter by equality you \\nshould use === (equal) or =!= (not equal). You can also use not function and the equalTo method.\\n%scala\\nimport org.apache.spark.sql.functions.col\\n87\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 86, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='df.where(col(“InvoiceNo”) === 536365)\\n.select(“InvoiceNo”, “Description”)\\n.show(5, false)\\nPython keeps a more conventional notation.\\n%python\\nfrom pyspark.sql.functions import col\\ndf.where(col(“InvoiceNo”) != 536365)\\\\\\n.select(“InvoiceNo”, “Description”)\\\\\\n.show(5, False)\\nNow we mentioned that we can specify boolean expressions with multiple parts when we use and or or. In Spark you \\nshould always chain together and filters as a sequential filter.\\nThe reason for this is that even if boolean expressions are expressed serially (one after the other) Spark will flatten all \\nof these filters into one statement and perform the filter at the same time, creating the and statement for us. While \\nyou may specify your statements explicitly using and if you like, it’s often easier to reason about and to read if you \\nspecify them serially. or statements need to be specified in the same statement.\\n%scala\\nval priceFilter = col(“UnitPrice”) > 600\\nval descripFilter = col(“Description”).contains(“POSTAGE”)\\ndf.where(col(“StockCode”).isin(“DOT”))\\n.where(priceFilter.or(descripFilter))\\n.show(5)\\n%python\\nfrom pyspark.sql.functions import instr\\n88\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 87, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='priceFilter = col(“UnitPrice”) > 600\\ndescripFilter = instr(df.Description, “POSTAGE”) >= 1\\ndf.where(df.StockCode.isin(“DOT”))\\\\\\n.where(priceFilter | descripFilter)\\\\\\n.show(5)\\n%sql\\nSELECT\\n*\\nFROM dfTable\\nWHERE\\nStockCode in (“DOT”) AND\\n(UnitPrice > 600 OR\\ninstr(Description, “POSTAGE”) >= 1)\\nBoolean expressions are not just reserved to filters. In order to filter a DataFrame we can also just specify a  \\nboolean column.\\nval DOTCodeFilter = col(“StockCode”) === “DOT”\\nval priceFilter = col(“UnitPrice”) > 600\\nval descripFilter = col(“Description”).contains(“POSTAGE”)\\ndf.withColumn(“isExpensive”,\\nDOTCodeFilter.and(priceFilter.or(descripFilter)))\\n.where(“isExpensive”)\\n.select(“unitPrice”, “isExpensive”)\\n.show(5)\\n%python\\nfrom pyspark.sql.functions import instr\\nDOTCodeFilter = col(“StockCode”) == “DOT”\\npriceFilter = col(“UnitPrice”) > 600\\ndescripFilter = instr(col(“Description”), “POSTAGE”) >= 1\\n89\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 88, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='df.withColumn(“isExpensive”,\\nDOTCodeFilter & (priceFilter | descripFilter))\\\\\\n.where(“isExpensive”)\\\\\\n.select(“unitPrice”, “isExpensive”)\\\\\\n.show(5)\\n%sql\\nSELECT\\nUnitPrice,\\n(StockCode = ‘DOT’ AND\\n(UnitPrice > 600 OR\\ninstr(Description, “POSTAGE”) >= 1)) as isExpensive\\nFROM dfTable\\nWHERE\\n(StockCode = ‘DOT’ AND\\n(UnitPrice > 600 OR\\ninstr(Description, “POSTAGE”) >= 1))\\nNotice how we did not have to specify our filter as an expression and how we could use a column name without any \\nextra work.\\nIf you’re coming from a SQL background all of these statements should seem quite familiar. Indeed, all of them can \\nbe expressed as a where clause. In fact, it’s often easier to just express filters as SQL statements than using the \\nprogrammatic DataFrame interface and Spark SQL allows us to do this without paying any performance penalty. For \\nexample, the two following statements are equivalent.\\nimport org.apache.spark.sql.functions.{expr, not, col}\\ndf.withColumn(“isExpensive”, not(col(“UnitPrice”).leq(250)))\\n.filter(“isExpensive”)\\n.select(“Description”, “UnitPrice”).show(5)\\ndf.withColumn(“isExpensive”, expr(“NOT UnitPrice <= 250”))\\n.filter(“isExpensive”)\\n.select(“Description”, “UnitPrice”).show(5)\\n%python\\n90\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 89, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='from pyspark.sql.functions import expr\\ndf.withColumn(“isExpensive”, expr(“NOT UnitPrice <= 250”))\\\\\\n.where(“isExpensive”)\\\\\\n.select(“Description”, “UnitPrice”).show(5)\\nWorking with Numbers\\nWhen working with big data, the second most common task you will do after filtering things is counting things. For \\nthe most part, we simply need to express our computation and that should be valid assuming we’re working with \\nnumerical data types.\\nTo fabricate a contrived example, let’s imagine that we found out that we misrecorded the quantity in our retail \\ndataset and true quantity is equal to (the current quantity * the unit price) ˆ 2 + 5. This will introduce our first \\nnumerical function as well the pow function that raises a column to the expressed power.\\n%scala\\nimport org.apache.spark.sql.functions.{expr, pow}\\nval fabricatedQuantity = pow(col(“Quantity”) * col(“UnitPrice”), 2) + 5\\ndf.select(\\nexpr(“CustomerId”),\\nfabricatedQuantity.alias(“realQuantity”))\\n.show(2)\\n%python\\nfrom pyspark.sql.functions import expr, pow\\nfabricatedQuantity = pow(col(“Quantity”) * col(“UnitPrice”), 2) + 5\\ndf.select(\\nexpr(“CustomerId”),\\nfabricatedQuantity.alias(“realQuantity”))\\\\\\n.show(2) \\n91\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 90, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='You’ll notice that we were able to multiply our columns together because they were both numerical. Naturally we can \\nadd and subtract as necessary as well. In fact we can do all of this a SQL expression as well.\\n%scala\\ndf.selectExpr(\\n“CustomerId”,\\n“(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity”)\\n.show(2)\\n%python\\ndf.selectExpr(\\n“CustomerId”,\\n“(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity” )\\\\\\n.show(2)\\n%sql\\nSELECT\\ncustomerId,\\n(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\\nFROM dfTable\\nAnother common numerical task is rounding. Now if you’d like to just round to a whole number, often times you can \\ncast it to an integer and that will work just fine. However Spark also has more detailed functions for performing this \\nexplicitly and to a certain level of precision. In this case we will round to one decimal place.\\n%scala\\nimport org.apache.spark.sql.functions.{round, bround}\\ndf.select(\\nround(col(“UnitPrice”), 1).alias(“rounded”),\\ncol(“UnitPrice”))\\n.show(5)\\n92\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 91, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='By default, the round function will round up if you’re exactly in between two numbers. You can round down with  \\nthe bround.\\n%scala\\nimport org.apache.spark.sql.functions.lit\\ndf.select(\\nround(lit(“2.5”)),\\nbround(lit(“2.5”)))\\n.show(2)\\n%python\\nfrom pyspark.sql.functions import lit, round, bround\\ndf.select(\\nround(lit(“2.5”)),\\nbround(lit(“2.5”)))\\\\\\n.show(2)\\n%sql\\nSELECT\\nround(2.5),\\nbround(2.5)\\nAnother numerical task is to compute the correlation of two columns. For example, we can see the Pearson \\nCorrelation Coefficient for two columns to see if cheaper things are typically bought in greater quantities. We can do \\nthis through a function as well as through the DataFrame statistic methods.\\n%scala\\nimport org.apache.spark.sql.functions.{corr}\\ndf.stat.corr(“Quantity”, “UnitPrice”)\\ndf.select(corr(“Quantity”, “UnitPrice”)).show()\\n%python\\n93\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 92, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='from pyspark.sql.functions import corr\\ndf.stat.corr(“Quantity”, “UnitPrice”)\\ndf.select(corr(“Quantity”, “UnitPrice”)).show()\\n%sql\\nSELECT\\ncorr(Quantity, UnitPrice)\\nFROM\\ndfTable\\nA common task is to compute summary statistics for a column or set of columns. We can use the describe method \\nto achieve exactly this. This will take all numeric columns and calculate the count, mean, standard deviation, min, and \\nmax. This should be used primarily for viewing in the console as the schema may change in the future.\\n%scala\\ndf.describe().show()\\n%python\\ndf.describe().show()\\nSummary\\nQuantity\\nUnitPrice\\nCustomerID\\ncount\\nmean\\nstddev\\nmin\\nmax\\n3108\\n8.627413127413128\\n26.371821677029203\\n-24\\n600\\n3108\\n4.151946589446603\\n15.638659854603892\\n0.0\\n607.49\\n1968\\n15661.388719512195\\n1854.4496996893627\\n12431.0\\n18229.0\\nIf you need these exact numbers you can also perform this as an aggregation yourself by importing the functions  \\nand applying them to the columns that you need.\\n94\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 93, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nimport org.apache.spark.sql.functions.{count, mean, stddev_pop, min, max}\\n%python\\nfrom pyspark.sql.functions import count, mean, stddev_pop, min, max\\nThere are a number of statistical functions available in the StatFunctions Package. These are DataFrame methods \\nthat allow you to calculate a vareity of different things. For instance, we can calculate either exact or approximate \\nquantiles of our data using the approxQuantile method.\\n%scala\\nval colName = “UnitPrice”\\nval quantileProbs = Array(0.5)\\nval relError = 0.05\\ndf.stat.approxQuantile(“UnitPrice”, quantileProbs, relError)\\n%python\\ncolName = “UnitPrice”\\nquantileProbs = [0.5]\\nrelError = 0.05\\ndf.stat.approxQuantile(“UnitPrice”, quantileProbs, relError)\\nWe can also use this to see a cross tabulation or frequent item pairs (Be careful, this output will be large).\\n%scala\\ndf.stat.crosstab(“StockCode”, “Quantity”).show()\\n%python\\ndf.stat.crosstab(“StockCode”, “Quantity”).show()\\n%scala\\ndf.stat.freqItems(Seq(“StockCode”, “Quantity”)).show()\\n95\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 94, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%python\\ndf.stat.freqItems([“StockCode”, “Quantity”]).show()\\nSpark is home to a variety of other features and functionality. For example, you can use Spark to construct a Bloom \\nFilter or Count Min Sketch using the stat sub-package. There are also a multitude of other functions available that \\nare self-explanatory and need not be explained individually.\\nWorking with Strings\\nString manipulation shows up in nearly every data flow and its worth explaining what you can do with strings. You \\nmay be manipulating log files performing regular expression extraction or substitution, or checking for simple string \\nexistence, or simply making all strings upper or lower case.\\nWe will start with the last task as it’s one of the simplest. The initcap function will capitalize every word in a given \\nstring when that word is separated from another via whitespace.\\n%scala\\nimport org.apache.spark.sql.functions.{initcap}\\ndf.select(initcap(col(“Description”))).show(2, false)\\n%python\\nfrom pyspark.sql.functions import initcap\\ndf.select(initcap(col(“Description”))).show()\\n%sql\\nSELECT\\ninitcap(Description)\\nFROM\\ndfTable\\nAs mentioned above, we can also quite simply lower case and upper case strings  as well.\\n96\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 95, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nimport org.apache.spark.sql.functions.{lower, upper}\\ndf.select(\\ncol(“Description”),\\nlower(col(“Description”)),\\nupper(lower(col(“Description”))))\\n.show(2)\\n%python\\nfrom pyspark.sql.functions import lower, upper\\ndf.select(\\ncol(“Description”),\\nlower(col(“Description”)),\\nupper(lower(col(“Description”))))\\\\\\n.show(2)\\n%sql\\nSELECT\\nDescription,\\nlower(Description),\\nUpper(lower(Description))\\nFROM\\ndfTable\\nAnother trivial task is adding or removing whitespace around a string. We can do this with lpad, ltrim, rpad and \\nrtrim, trim.\\n%scala\\nimport org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}\\ndf.select(\\nltrim(lit(“ HELLO “)).as(“ltrim”),\\nrtrim(lit(“ HELLO “)).as(“rtrim”),\\ntrim(lit(“ HELLO “)).as(“trim”),\\n97\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 96, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='lpad(lit(“HELLO”), 3, “ “).as(“lp”),\\nrpad(lit(“HELLO”), 10, “ “).as(“rp”))\\n.show(2)\\n%python\\nfrom pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\\ndf.select(\\nltrim(lit(“ HELLO “)).alias(“ltrim”),\\nrtrim(lit(“ HELLO “)).alias(“rtrim”),\\ntrim(lit(“ HELLO “)).alias(“trim”),\\nlpad(lit(“HELLO”), 3, “ “).alias(“lp”),\\nrpad(lit(“HELLO”), 10, “ “).alias(“rp”))\\\\\\n.show(2)\\n%sql\\nSELECT\\nltrim(‘ HELLLOOOO ‘),\\nrtrim(‘ HELLLOOOO ‘),\\ntrim(‘ HELLLOOOO ‘),\\nlpad(‘HELLOOOO ‘, 3, ‘ ‘),\\nrpad(‘HELLOOOO ‘, 10, ‘ ‘)\\nFROM\\ndfTable\\n1trim\\ntrim\\nrtrim\\n1p\\nrp\\nHELLO\\nHELLO\\nHELLO\\nHELLO\\nHELLO\\nHELLO\\nHE\\nHE\\nHELLO\\nHELLO\\n only showing top 2 rows\\nYou’ll notice that if lpad or rpad takes a number less than the length of the string, it will always remove values from \\nthe right side of the string.\\n98\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 97, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Regular Expressions\\nProbably one of the most frequently performed tasks is searching for the existance of one string on another or \\nreplacing all mentions of a string with another value. This is often done with a tool called “Regular Expressions” that \\nexist in many programming languages. Regular expressions give the user an ability to specify a set of rules to use to \\neither extract values from a string or replace them with some other values.\\nSpark leverages the complete power of Java Regular Expressions. The syntax departs slightly from other programming \\nlanguages so it is worth reviewing before putting anything into production.. There are two key functions in Spark that \\nyou’ll need to perform regular expression tasks: regexp_extract and regexp_replace. These functions extract \\nvalues and replace values respectively.\\nLet’s explore how to use the regexp_replace function to replace substitute colors names in our description \\ncolumn.\\n%scala\\nimport org.apache.spark.sql.functions.regexp_replace\\nval simpleColors = Seq(“black”, “white”, “red”, “green”, “blue”)\\nval regexString = simpleColors.map(_.toUpperCase).mkString(“|”)\\n// the | signifies `OR` in regular expression syntax\\ndf.select(\\nregexp_replace(col(“Description”), regexString, “COLOR”)\\n.alias(“color_cleaned”),\\ncol(“Description”))\\n.show(2)\\n%python\\nfrom pyspark.sql.functions import regexp_replace\\nregex_string = “BLACK|WHITE|RED|GREEN|BLUE”\\ndf.select(\\nregexp_replace(col(“Description”), regex_string, “COLOR”)\\n.alias(“color_cleaned”),\\ncol(“Description”))\\\\\\n.show(2)\\n99\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 98, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%sql\\nSELECT\\nregexp_replace(Description, ‘BLACK|WHITE|RED|GREEN|BLUE’, ‘COLOR’) as  \\ncolor_cleaned,\\nDescription\\nFROM\\ndfTable\\n color_cleaned \\nDescription\\nCOLOR HANGING HEA...\\n COLOR METAL LANTERN\\nWHITE HANGING HEA...\\n  WHITE METAL LANTERN\\nAnother task may be to replace given characters with other characters. Building this as regular expression could be \\ntedious so Spark also provides the translate function to replace these values. This is done at the character level and \\nwill replace all instances of a character with the indexed character in the replacement string.\\n%scala\\nimport org.apache.spark.sql.functions.translate\\ndf.select(\\ntranslate(col(“Description”), “LEET”, “1337”),\\ncol(“Description”))\\n.show(2)\\n%python\\nfrom pyspark.sql.functions import translate\\ndf.select(\\ntranslate(col(“Description”), “LEET”, “1337”),\\ncol(“Description”))\\\\\\n.show(2)\\n%sql\\n100\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 99, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='SELECT\\ntranslate(Description, ‘LEET’, ‘1337’),\\nDescription\\nFROM\\ndfTable\\n translate(Description, LEET, 1337)|\\nDescription\\nWHI73 HANGING H3A...\\n  WHI73 M37A1 1AN73RN\\nWHITE HANGING HEA...\\n  WHITE METAL LANTERN\\nWe can also perform something similar like pulling out the first mentioned color.\\n%scala\\nimport org.apache.spark.sql.functions.regexp_extract\\nval regexString = simpleColors\\n.map(_.toUpperCase)\\n.mkString(“(“, “|”, “)”)\\n// the | signifies OR in regular expression syntax\\ndf.select(\\nregexp_extract(col(“Description”), regexString, 1)\\n.alias(“color_cleaned”),\\ncol(“Description”))\\n.show(2)\\n%python\\nfrom pyspark.sql.functions import regexp_extract\\nextract_str = “(BLACK|WHITE|RED|GREEN|BLUE)”\\ndf.select(\\nregexp_extract(col(“Description”), extract_str, 1)\\n.alias(“color_cleaned”),\\n101\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 100, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='col(“Description”))\\\\\\n.show(2)\\n%sql\\nSELECT\\nregexp_extract(Description, ‘(BLACK|WHITE|RED|GREEN|BLUE)’, 1),\\nDescription\\nFROM\\ndfTable\\nSometimes, rather than extracting values, we simply want to check for existence. We can do this with the contains \\nmethod on each column. This will return a boolean declaring whether it can find that string in the column’s string.\\n%scala\\nval containsBlack = col(“Description”).contains(“BLACK”)\\nval containsWhite = col(“DESCRIPTION”).contains(“WHITE”)\\ndf.withColumn(“hasSimpleColor”, containsBlack.or(containsWhite))\\n.filter(“hasSimpleColor”)\\n.select(“Description”)\\n.show(3, false)\\nIn Python we can use the instr function.\\n%python\\nfrom pyspark.sql.functions import instr\\ncontainsBlack = instr(col(“Description”), “BLACK”) >= 1\\ncontainsWhite = instr(col(“Description”), “WHITE”) >= 1\\ndf.withColumn(“hasSimpleColor”, containsBlack | containsWhite)\\\\\\n.filter(“hasSimpleColor”)\\\\\\n.select(“Description”)\\\\\\n.show(3, False)\\n102\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 101, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%sql\\nSELECT\\nDescription\\nFROM\\ndfTable\\nWHERE\\ninstr(Description, ‘BLACK’) >= 1 OR\\ninstr(Description, ‘WHITE’) >= 1\\n Description\\nWHITE HANGING HEART T-LIGHT HOLDER\\nWHITE METAL LANTERN \\nRED WOOLLY HOTTIE WHITE HEART.\\nonly showing top 3 rows\\nThis is trivial with just two values but gets much more complicated with more values. \\nLet’s work through this in a more dynamic way and take advantage of Spark’s ability to accept a dynamic number \\nof arguments. When we convert a list of values into a set of arguments and pass them into a function, we use a \\nlanguage feature called varargs. This feature allows us to effectively unravel an array of arbitrary length and pass it as \\narguments to a function. This, coupled with select allows us to create arbitrary numbers of columns dynamically.\\n%scala\\nval simpleColors = Seq(“black”, “white”, “red”, “green”, “blue”)\\nval selectedColumns = simpleColors.map(color => {\\ncol(“Description”)\\n.contains(color.toUpperCase)\\n.alias(s”is_$color”)\\n}):+expr(“*”) // could also append this value\\n103\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 102, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='df\\n.select(selectedColumns:_*)\\n.where(col(“is_white”).or(col(“is_red”)))\\n.select(“Description”)\\n.show(3, false)\\n Description\\nWHITE HANGING HEART T-LIGHT HOLDER\\nWHITE METAL LANTERN \\nRED WOOLLY HOTTIE WHITE HEART.\\nWe can also do this quite easily in Python. In this case we’re going to use a different function locate that returns the \\ninteger location (1 based location). We then convert that to a boolean before using it as a the same basic feature.\\n%python\\nfrom pyspark.sql.functions import expr, locate\\nsimpleColors = [“black”, “white”, “red”, “green”, “blue”]\\ndef color_locator(column, color_string):\\n“””This function creates a column declaring whether or\\nnot a given pySpark column contains the UPPERCASED\\ncolor.\\nReturns a new column type that can be used\\nin a select statement.\\n“””\\nreturn locate(color_string.upper(), column)\\\\\\n.cast(“boolean”)\\\\\\n.alias(“is_” + c)\\nselectedColumns = [color_locator(df.Description, c) for c in simpleColors]\\nselectedColumns.append(expr(“*”)) # has to a be Column type\\n104\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 103, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='df\\\\\\n.select(*selectedColumns)\\\\\\n.where(expr(“is_white OR is_red”))\\\\\\n.select(“Description”)\\\\\\n.show(3, False)\\nThis simple feature is often one that can help you programmatically generate columns or boolean filters in a way that \\nis simple to reason about and extend. We could extend this to calculating the smallest common denominator for a \\ngiven input value or whether or not a number is a prime.\\nWorking with Dates and Timestamps\\nDates and times are a constant challenge in programming languages and databases. It’s always necessary to keep \\ntrack of timezones and make sure that formats are correct and valid. Spark does its best to keep things simple by \\nfocusing explicitly on two kinds of time related information. There are dates, which focus exclusively on calendar \\ndates, and timestamps that include both date and time information.\\nNow as we hinted at above, working with dates and timestamps closely relates to working with strings because we \\noften store our timestamps or dates as strings and convert them into date types at runtime. This is less common when \\nworking with databases and structured data but much more common when we are working with text and csv files.\\nNow Spark, as we saw with our current dataset, will make a best effort to correctly identify column types, including \\ndates and timestamps when we enable inferSchema. We can see that this worked quite well with our current \\ndataset because it was able to identify and read our date format without us having to provide some specification  \\nfor it.\\ndf.printSchema()\\nroot\\n|-- InvoiceNo: string (nullable = true)\\n|-- StockCode: string (nullable = true)\\n|-- Description: string (nullable = true)\\n|-- Quantity: integer (nullable = true)\\n|-- InvoiceDate: timestamp (nullable = true)\\n|-- UnitPrice: double (nullable = true)\\n|-- CustomerID: double (nullable = true)\\n|-- Country: string (nullable = true)\\n105\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 104, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='While Spark will do this on a best effort basis, sometimes there will be no getting around working with strangely \\nformatted dates and times. Now the key to reasoning about the transformations that you are going to need to apply \\nis to ensure that you know exactly what type and format you have at each given step of the way. Another common \\ngotcha is that Spark’s TimestampType only supports second-level precision, this means that if you’re going to \\nbe working with milliseconds or microseconds, you’re going to have to work around this problem by potentially \\noperating on them as longs. Any more precision when coercing to a TimestampType will be removed.\\nSpark can be a bit particular about what format you have at any given point in time. It’s important to be explicit when \\nparsing or converting to make sure there are no issues in doing so. At the end of the day, Spark is working with Java \\ndates and timestamps and therefore conforms to those standards. Let’s start with the basics and get the current date \\nand the current timestamps.\\n%scala\\nimport org.apache.spark.sql.functions.{current_date, current_timestamp}\\nval dateDF = spark.range(10)\\n.withColumn(“today”, current_date())\\n.withColumn(“now”, current_timestamp())\\ndateDF.createOrReplaceTempView(“dateTable”)\\n%python\\nfrom pyspark.sql.functions import current_date, current_timestamp\\ndateDF = spark.range(10)\\\\\\n.withColumn(“today”, current_date())\\\\\\n.withColumn(“now”, current_timestamp())\\ndateDF.createOrReplaceTempView(“dateTable”)\\ndateDF.printSchema()\\nNow that we have a simple DataFrame to work with, let’s add and subtract 5 days from today. These functions take a \\ncolumn and then the number of days to either add or subtract as the arguments.\\n%scala\\nimport org.apache.spark.sql.functions.{date_add, date_sub}\\n106\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 105, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='dateDF\\n.select(\\ndate_sub(col(“today”), 5),\\ndate_add(col(“today”), 5))\\n.show(1)\\n%python\\nfrom pyspark.sql.functions import date_add, date_sub\\ndateDF\\\\\\n.select(\\ndate_sub(col(“today”), 5),\\ndate_add(col(“today”), 5))\\\\\\n.show(1)\\n%sql\\nSELECT\\ndate_sub(today, 5),\\ndate_add(today, 5)\\nFROM\\ndateTable\\nAnother common task is to take a look at the difference between two dates. We can do this with the datediff \\nfunction that will return the number of days in between two dates. Most often we just care about the days although \\nsince months can have a strange number of days there also exists a function months_between that gives you the \\nnumber of months between two dates.\\n%scala\\nimport org.apache.spark.sql.functions.{datediff, months_between, to_date}\\ndateDF\\n.withColumn(“week_ago”, date_sub(col(“today”), 7))\\n.select(datediff(col(“week_ago”), col(“today”)))\\n.show(1)\\ndateDF\\n.select(\\n107\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 106, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='to_date(lit(“2016-01-01”)).alias(“start”),\\nto_date(lit(“2017-05-22”)).alias(“end”))\\n.select(months_between(col(“start”), col(“end”)))\\n.show(1)\\n%python\\nfrom pyspark.sql.functions import datediff, months_between, to_date\\ndateDF\\\\\\n.withColumn(“week_ago”, date_sub(col(“today”), 7))\\\\\\n.select(datediff(col(“week_ago”), col(“today”)))\\\\\\n.show(1)\\ndateDF\\\\\\n.select(\\nto_date(lit(“2016-01-01”)).alias(“start”),\\nto_date(lit(“2017-05-22”)).alias(“end”))\\\\\\n.select(months_between(col(“start”), col(“end”)))\\\\\\n.show(1)\\n%sql\\nSELECT\\nto_date(‘2016-01-01’),\\nmonths_between(‘2016-01-01’, ‘2017-01-01’),\\ndatediff(‘2016-01-01’, ‘2017-01-01’)\\nFROM\\ndateTable\\nYou’ll notice that I introduced a new function above, the to_date function. This function allows us to convert a date \\nof the format “2017-01-01” to a Spark date. Of course, for this to work our date must be in the year-month-day \\nformat. You’ll notice that in order to perform this I’m also using the lit function which ensures that we’re returning a \\nliteral value in our expression not trying to evaluate subtraction.\\n%scala\\nimport org.apache.spark.sql.functions.{to_date, lit}\\n108\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 107, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='spark.range(5).withColumn(“date”, lit(“2017-01-01”))\\n.select(to_date(col(“date”)))\\n.show(1)\\n%python\\nfrom pyspark.sql.functions import to_date, lit\\nspark.range(5).withColumn(“date”, lit(“2017-01-01”))\\\\\\n.select(to_date(col(“date”)))\\\\\\n.show(1)\\nWARNING \\nSpark will not throw an error if it cannot parse the date, it’ll just return null. This can be a bit tricky in larger \\npipelines because you may be expecting your data in one format and getting it in another. To illustrate, let’s take \\na look at the date format that has switched from year-month-day to year-day-month. Spark will fail to parse this \\ndate and silently return null instead.\\ndateDF.select(to_date(lit(“2016-20-12”)),to_date(lit(“2017-12-11”))).show(1)\\n to_date(2016-20-12)\\nto_date(2017-12-11)\\nnull\\n2017-12-11\\nWe find this to be an especially tricky situation for bugs because some dates may match the correct format while \\nothers do not. See how above, the second date is show to be Decembers 11th instead of the correct day, November \\n12th? Spark doesn’t throw an error because it cannot know whether the days are mixed up or if that specific row is \\nincorrect.\\nLet’s fix this pipeline, step by step and come up with a robust way to avoid these issues entirely. The first step is \\nto remember that we need to specify our date format according to the Java SimpleDateFormat standard as \\ndocumented in https: //docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html.\\n109\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 108, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='By using the unix_timestamp we can parse our date into a bigInt that specifies the Unix timestamp in seconds. \\nWe can then cast that to a literal timestamp before passing that into the to_date format which accepts \\ntimestamps, strings, and other dates.\\nimport org.apache.spark.sql.functions.{unix_timestamp, from_unixtime}\\nval dateFormat = “yyyy-dd-MM”\\nval cleanDateDF = spark.range(1)\\n.select(\\nto_date(unix_timestamp(lit(“2017-12-11”), dateFormat).cast(“timestamp”))\\n.alias(“date”),\\nto_date(unix_timestamp(lit(“2017-20-12”), dateFormat).cast(“timestamp”))\\n.alias(“date2”))\\ncleanDateDF.createOrReplaceTempView(“dateTable2”)\\n%python\\nfrom pyspark.sql.functions import unix_timestamp, from_unixtime\\ndateFormat = “yyyy-dd-MM”\\ncleanDateDF = spark.range(1)\\\\\\n.select(\\nto_date(unix_timestamp(lit(“2017-12-11”), dateFormat).cast(“timestamp”))\\\\\\n.alias(“date”),\\nto_date(unix_timestamp(lit(“2017-20-12”), dateFormat).cast(“timestamp”))\\\\\\n.alias(“date2”))\\ncleanDateDF.createOrReplaceTempView(“dateTable2”)\\n%sql\\nSELECT\\nto_date(cast(unix_timestamp(date, ‘yyyy-dd-MM’) as timestamp)),\\nto_date(cast(unix_timestamp(date2, ‘yyyy-dd-MM’) as timestamp)),\\nto_date(date)\\nFROM\\ndateTable2\\nThe above example code also shows us how easy it is to cast between timestamps and dates.\\n110\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 109, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\ncleanDateDF\\n.select(\\nunix_timestamp(col(“date”), dateFormat).cast(“timestamp”))\\n.show()\\n%python\\ncleanDateDF\\\\\\n.select(\\nunix_timestamp(col(“date”), dateFormat).cast(“timestamp”))\\\\\\n.show()\\nOnce we’ve gotten our date or timestamp into the correct format and type,Comparing between them is actually quite \\neasy. We just need to be sure to either use a date/timestamp type or specify our string according to the right format of \\nyyyy-MM-dd if we’re comparing a date.\\ncleanDateDF.filter(col(“date2”) > lit(“2017-12-12”)).show()\\nOne minor point is that we can also set this as a string which Spark parses to a literal.\\ncleanDateDF.filter(col(“date2”) > “’2017-12-12’”).show()\\nWorking with Nulls in Data\\nAs a best practice, you should always use nulls to represent missing or empty data in your DataFrames. Spark can \\noptimize working with null values more than it can if you use empty strings or other values. The primary way of \\ninteracting with null values, at DataFrame scale, is to use the .na subpackage on a DataFrame.\\nIn Spark there are two things you can do with null values. You can explicitly drop nulls or you can fill them with a value \\n(globally or on a per column basis).\\nLet’s experiment with each of these now.\\n111\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 110, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Drop\\nThe simplest is probably drop, which simply removes rows that contain nulls. The default is to drop any row where \\nany value is null.\\ndf.na.drop()\\ndf.na.drop(“any”)\\nIn SQL we have to do this column by column.\\n%sql\\nSELECT\\n*\\nFROM\\ndfTable\\nWHERE\\nDescription IS NOT NULL\\nPassing in “any” as an argument will drop a row if any of the values are null. Passing in “all” will only drop the row if all \\nvalues are null or NaN for that row.\\ndf.na.drop(“all”)\\nWe can also apply this to certain sets of columns by passing in an array of columns.\\n%scala\\ndf.na.drop(“all”, Seq(“StockCode”, “InvoiceNo”))\\n%python\\ndf.na.drop(“all”, subset=[“StockCode”, “InvoiceNo”])\\n112\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 111, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Fill\\nFill allows you to fill one or more columns with a set of values. This can be done by specifying a map, specific value \\nand a set of columns.\\nFor example to fill all null values in String columns I might specify.\\ndf.na.fill(“All Null values become this string”)\\nWe could do the same for integer columns with df.na.fill(5:Integer) or for Doubles df.na.\\nfill(5:Double). In order to specify columns, we just pass in an array of column names like we did above.\\n%scala\\ndf.na.fill(5, Seq(“StockCode”, “InvoiceNo”))\\n%python\\ndf.na.fill(“all”, subset=[“StockCode”, “InvoiceNo”])\\nWe can also do with with a Scala Map where the key is the column name and the value is the value we would like to \\nuse to fill null values.\\n%scala\\nval fillColValues = Map(\\n“StockCode” -> 5,\\n“Description” -> “No Value”\\n)\\ndf.na.fill(fillColValues)\\n%python\\n113\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 112, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='fill_cols_vals = {\\n“StockCode”: 5,\\n“Description” : “No Value”\\n}\\ndf.na.fill(fill_cols_vals)\\nReplace\\nIn addition to replacing null values like we did with drop and fill, there are more flexible options that we can use \\nwith more than just null values. Probably the most common use case is to replace all values in a certain column \\naccording to their current value. The only requirement is that this value be the same type as the original value.\\n%scala\\ndf.na.replace(“Description”, Map(“” -> “UNKNOWN”))\\n%python\\ndf.na.replace([“”], [“UNKNOWN”], “Description”)\\nWorking with Complex Types\\nComplex types can help you organize and structure your data in ways that make more sense for the problem you are \\nhoping to solve. There are three kinds of complex types, structs, arrays, and maps.\\nStructs\\nYou can think of structs as DataFrames within DataFrames. A worked example will illustrate this more clearly. We can \\ncreate a struct by wrapping a set of columns in parenthesis in a query.\\ndf.selectExpr(“(Description, InvoiceNo) as complex”, “*”)\\ndf.selectExpr(“struct(Description, InvoiceNo) as complex”, “*”)\\n114\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 113, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nimport org.apache.spark.sql.functions.struct\\nval complexDF = df\\n.select(struct(“Description”, “InvoiceNo”).alias(“complex”))\\ncomplexDF.createOrReplaceTempView(“complexDF”)\\n%python\\nfrom pyspark.sql.functions import struct\\ncomplexDF = df\\\\\\n.select(struct(“Description”, “InvoiceNo”).alias(“complex”))\\ncomplexDF.createOrReplaceTempView(“complexDF”)\\nWe now have a DataFrame with a column complex. We can query it just as we might another DataFrame, the only \\ndifference is that we use a dot syntax to do so.\\ncomplexDF.select(“complex.Description”)\\nWe can also query all values in the struct with *. This brings up all the columns to the top level DataFrame.\\ncomplexDF.select(“complex.*”)\\n%sql\\nSELECT\\ncomplex.*\\nFROM\\ncomplexDF\\nArrays\\nTo define arrays, let’s work through a use case. With our current data, our object is to take every single word in our \\nDescription column and convert that into a row in our DataFrame.\\n115\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 114, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='The first task is to turn our Description column into a complex type, an array.\\nsplit\\nWe do this with the split function and specify the delimiter.\\n%scala\\nimport org.apache.spark.sql.functions.split\\ndf.select(split(col(“Description”), “ “)).show(2)\\n%python\\nfrom pyspark.sql.functions import split\\ndf.select(split(col(“Description”), “ “)).show(2)\\n%sql\\nSELECT\\nsplit(Description, ‘ ‘)\\nFROM\\ndfTable\\nThis is quite powerful because Spark will allow us to manipulate this complex type as another column. We can also \\nquery the values of the array with a python-like syntax.\\n%scala\\ndf.select(split(col(“Description”), “ “).alias(“array_col”))\\n.selectExpr(“array_col[0]”)\\n.show(2)\\n%python\\ndf.select(split(col(“Description”), “ “).alias(“array_col”))\\\\\\n.selectExpr(“array_col[0]”)\\\\\\n.show(2)\\n116\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 115, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%sql\\nSELECT\\nsplit(Description, ‘ ‘)[0]\\nFROM\\ndfTable\\nArray Contains\\nFor instance we can see if this array contains a value.\\nimport org.apache.spark.sql.functions.array_contains\\ndf.select(array_contains(split(col(“Description”), “ “), “WHITE”)).show(2)\\n%python\\nfrom pyspark.sql.functions import array_contains\\ndf.select(array_contains(split(col(“Description”), “ “), “WHITE”)).show(2)\\n%sql\\nSELECT\\narray_contains(split(Description, ‘ ‘), ‘WHITE’)\\nFROM\\ndfTable\\nHowever this does not solve our current problem. In order to convert a complex type into a set of rows (one per value \\nin our array), we use the explode function.\\nExplode\\nThe explode function takes a column that consists of arrays and creates one row (with the rest of the values \\nduplicated) per value in the array. The following figure illustrates the process.\\n117\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 116, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nimport org.apache.spark.sql.functions.{split, explode}\\ndf.withColumn(“splitted”, split(col(“Description”), “ “))\\n.withColumn(“exploded”, explode(col(“splitted”)))\\n.select(“Description”, “InvoiceNo”, “exploded”)\\n%python\\nfrom pyspark.sql.functions import split, explode\\ndf.withColumn(“splitted”, split(col(“Description”), “ “))\\\\\\n.withColumn(“exploded”, explode(col(“splitted”)))\\\\\\n.select(“Description”, “InvoiceNo”, “exploded”)\\\\\\nMaps\\nMaps are used less frequently but are still important to cover. We create them with the map function and key value \\npairs of columns. Then we can select them just like we might select from an array.\\nimport org.apache.spark.sql.functions.map\\ndf.select(map(col(“Description”), col(“InvoiceNo”)).alias(“complex_map”))\\n.selectExpr(“complex_map[‘Description’]”)\\nFigure 1:\\n“Hello World”  ,  “other col” \\nSplit\\nExplode\\n[ “Hello” , “World” ] , “other col”\\n “Hello” , “other col” \\n “World” , “other col” \\n118\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 117, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%sql\\nSELECT\\nmap(Description, InvoiceNo) as complex_map\\nFROM\\ndfTable\\nWHERE\\nDescription IS NOT NULL\\nWe can also explode map types which will turn them into columns.\\nimport org.apache.spark.sql.functions.map\\ndf.select(map(col(“Description”), col(“InvoiceNo”)).alias(“complex_map”))\\n.selectExpr(“explode(complex_map)”)\\n.take(5)\\nWorking with JSON\\nSpark has some unique support for working with JSON data. You can operate directly on strings of JSON in Spark and \\nparse from JSON or extract JSON objects. Let’s start by creating a JSON column.\\n%scala\\nval jsonDF = spark.range(1)\\n.selectExpr(“””\\n‘{“myJSONKey” : {“myJSONValue” : [1, 2, 3]}}’ as jsonString\\n“””)\\n119\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 118, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%python\\njsonDF = spark.range(1)\\\\\\n.selectExpr(“””\\n‘{“myJSONKey” : {“myJSONValue” : [1, 2, 3]}}’ as jsonString\\n“””)\\nWe can use the get_json_object to inline query a JSON object, be it a dictionary or array. We can use json_\\ntuple if this object has only one level of nesting.\\n%scala\\nimport org.apache.spark.sql.functions.{get_json_object, json_tuple}\\njsonDF.select(\\nget_json_object(col(“jsonString”), “$.myJSONKey.myJSONValue[1]”),\\njson_tuple(col(“jsonString”), “myJSONKey”))\\n.show()\\n%python\\nfrom pyspark.sql.functions import get_json_object, json_tuple\\njsonDF.select(\\nget_json_object(col(“jsonString”), “$.myJSONKey.myJSONValue[1]”),\\njson_tuple(col(“jsonString”), “myJSONKey”))\\\\\\n.show()\\nThe equivalent in SQL would be.\\njsonDF.selectExpr(“json_tuple(jsonString, ‘$.myJSONKey.myJSONValue[1]’) as res”)\\nWe can also turn a StructType into a JSON string using the to_json function.\\n120\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 119, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nimport org.apache.spark.sql.functions.to_json\\ndf.selectExpr(“(InvoiceNo, Description) as myStruct”)\\n.select(to_json(col(“myStruct”)))\\n%python\\nfrom pyspark.sql.functions import to_json\\ndf.selectExpr(“(InvoiceNo, Description) as myStruct”)\\\\\\n.select(to_json(col(“myStruct”)))\\nThis function also accepts a dictionary (map) of parameters that are the same as the JSON data source. We can use \\nthe from_json function to parse this (or other json) back in. This naturally requires us to specify a schema and \\noptionally we can specify a Map of options as well.\\n%scala\\nimport org.apache.spark.sql.functions.from_json\\nimport org.apache.spark.sql.types._\\nval parseSchema = new StructType(Array(\\nnew StructField(“InvoiceNo”,StringType,true),\\nnew StructField(“Description”,StringType,true)))\\ndf.selectExpr(“(InvoiceNo, Description) as myStruct”)\\n.select(to_json(col(“myStruct”)).alias(“newJSON”))\\n.select(from_json(col(“newJSON”), parseSchema), col(“newJSON”))\\n%python\\nfrom pyspark.sql.functions import from_json\\nfrom pyspark.sql.types import *\\nparseSchema = StructType((\\nStructField(“InvoiceNo”,StringType(),True),\\nStructField(“Description”,StringType(),True)))\\ndf.selectExpr(“(InvoiceNo, Description) as myStruct”)\\\\\\n.select(to_json(col(“myStruct”)).alias(“newJSON”))\\\\\\n.select(from_json(col(“newJSON”), parseSchema), col(“newJSON”))\\\\\\n121\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 120, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='User-Defined Functions\\nOne of the most powerful things that you can do in Spark is define your own functions. These allow you to write your \\nown custom transformations using Python or Scala and even leverage external libraries like numpy in doing so. These \\nfunctions are called user defined functions or UDFs and can take and return one or more columns as input. \\nSpark UDFs are incredibly powerful because they can be written in several different programming languages and do \\nnot have to be written in an esoteric format or DSL. They’re just functions that operate on the data, record by record.\\nWhile we can write our functions in Scala, Python, or Java, there are performance considerations that you should be \\naware of. To illustrate this, we’re going to walk through exactly what happens when you create UDF, pass that into \\nSpark, and then execute code using that UDF.\\nThe first step is the actual function, we’ll just a take a simple one for this example. We’ll write a power3 function that \\ntakes a number and raises it to a power of three.\\n%scala\\nval udfExampleDF = spark.range(5).toDF(“num”)\\ndef power3(number:Double):Double = {\\n   number * number * number\\n}\\npower3(2.0)\\n%python\\nudfExampleDF = spark.range(5).toDF(“num”)\\ndef power3(double_value):\\nreturn double_value ** 3\\npower3(2.0)\\nIn this trivial example, we can see that our functions work as expected. We are able to provide an individual input and \\nproduce the expected result (with this simple test case). Thus far our expectations for the input are high, it must be a \\nspecific type and cannot be a null value. See the section in this chapter titled “Working with Nulls in Data”.\\nNow that we’ve created these functions and tested them, we need to register them with Spark so that we can used \\nthem on all of our worker machines. Spark will serialize the function on the driver and transfer it over the network to \\nall executor processes. This happens regardless of language.\\n122\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 121, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='Once we go to use the function, there are essentially two different things that occur. If the function is written in Scala \\nor Java then we can use that function within the JVM. This means there will be little performance penalty aside from \\nthe fact that we can’t take advantage of code generation capabilities that Spark has for built-in functions. There can \\nbe performance issues if you create or use a lot of objects which we will cover in the optimization section.\\nIf the function is written in Python, something quite different happens. Spark will start up a python process on the \\nworker, serialize all of the data to a format that python can understand (remember it was in the JVM before), execute \\nthe function row by row on that data in the python process, before finally returning the results of the row operations \\nto the JVM and Spark.\\nWARNING: Starting up this Python process is expensive but the real cost is in serializing the data to Python. This is \\ncostly for two reasons, it is an expensive computation but also once the data enters Python, Spark cannot manage the \\nmemory of the worker. This means that you could potentially cause a worker to fail if it becomes resource constrained \\n(because both the JVM and python are competing for memory on the same machine). We recommend that you \\nwrite your UDFs in Scala - the small amount of time it should take you to write the function in Scala will always yield \\nsignificant speed ups and on top of that, you can still use the function from Python!\\nNow that we have an understanding of the process, let’s work through our example. First we need to register the \\nfunction to be available as a DataFrame function.\\nFigure 2:\\nSpark Session\\nExecutor processes\\nWorker python process\\nScala UDF\\nPython UDF\\n1.  Function serialized \\n      and sent to workers\\n3.  Python returns \\n      answer\\n2.  Spark starts Python process \\n      and sends data\\nDriver\\n123\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 122, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nimport org.apache.spark.sql.functions.udf\\nval power3udf = udf(power3(_:Double):Double)\\nNow we can use that just like any other DataFrame function.\\n%scala\\nudfExampleDF.select(power3udf(col(“num”))).show()\\nThe same applies to Python, we first register it.\\n%python\\nfrom pyspark.sql.functions import udf\\npower3udf = udf(power3)\\nThen we can use it in our DataFrame code.\\n%python\\nfrom pyspark.sql.functions import col\\nudfExampleDF.select(power3udf(col(“num”))).show()\\nNow as of now, we can only use this as DataFrame function. That is to say, we can’t use it within a string expression, \\nonly on an expression. However, we can also register this UDF as a Spark SQL function. This is valuable because it \\nmakes it simple to use this function inside of SQL as well as across languages.\\nLet’s register the function in Scala.\\n124\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 123, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='%scala\\nspark.udf.register(“power3”, power3(_:Double):Double)\\nudfExampleDF.selectExpr(“power3(num)”).show()\\nNow because this function is registered with Spark SQL, and we’ve learned that any Spark SQL function or epxression \\nis valid to use as an expression when working with DataFrames, we can turn around and use the UDF that we wrote in \\nScala, in Python. However rather than using it as a DataFrame function we use it as a SQL expression.\\n%python\\nudfExampleDF.selectExpr(“power3(num)”).show()\\n# registered in Scala\\nWe can also register our Python function to be available as SQL function and use that in any language as well.\\nOne thing we can also do to make sure that our functions are working correctly is specify a return type. As we saw in \\nthe beginning of this section, Spark manages its own type information that does not align exactly with Python’s types. \\nTherefore it’s a best practice to define the return type for your function when you define it. It is important to note that \\nspecifying the return type is not necessary but is a best practice.\\nIf you specify the type that doesn’t align with the actual type returned by the function - Spark will not error but rather \\njust return null to designate a failure. You can see this if you were to switch the return type in the below function to \\nbe a DoubleType.\\n%python\\nfrom pyspark.sql.types import IntegerType, DoubleType\\nspark.udf.register(“power3py”, power3, DoubleType())\\n%python\\nudfExampleDF.selectExpr(“power3py(num)”).show()\\n# registered via Python\\n125\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 124, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='This is because the range above creates Integers. When Integers are operated on in Python, Python won’t convert \\nthem into floats (the corresponding type to Spark’s Double type), therefore we see null. We can remedy this by \\nensuring our Python function returns a float instead of an Integer and the function will behave correctly.\\nNaturally we can use either of these from SQL too once we register them.\\n%sql\\nSELECT\\npower3py(12), -- doesn’t work because of return type\\npower3(12)\\nThis chapter demonstrated how easy it is to extend Spark SQL to your own purposes and do so in a way that is not \\nsome esoteric, domain-specific language but rather simple functions that are easy to test and maintain without even \\nusing Spark! This is an amazingly powerful tool we can use to specify sophisticated business logic that can run on 5 \\nrows on our local machines or on terabytes of data on a hundred node cluster!\\n126\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 125, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='The Unified Analytics Platform\\n \\n \\nThe datasets used in the book are also available for you to explore: \\nSpark: The Definitive Guide Datasets \\nTry Databricks for free \\ndatabricks.com/try-databricks \\nContact us for a personalized demo \\ndatabricks.com/contact-databricks \\n© Databricks 2017. All rights reserved. Apache Spark and the Apache Spark Logo are trademarks of the Apache Software Foundation.\\n127\\n', metadata={'source': 'spark.pdf', 'file_path': 'spark.pdf', 'page': 126, 'total_pages': 127, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Adobe InDesign CC 2017 (Macintosh)', 'producer': 'Adobe PDF Library 15.0', 'creationDate': \"D:20170626134039-07'00'\", 'modDate': \"D:20170626134116-07'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_api_key = os.getenv('ELASTICSEARCH_API_KEY')\n",
    "import elasticsearch\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "\n",
    "es_client= elasticsearch.Elasticsearch(\n",
    "    hosts=[\"https://e2e8bf7fd17c44a8a735f5cbd7632c15.us-east-2.aws.elastic-cloud.com:443\"],\n",
    "    api_key=elastic_api_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ElasticsearchStore(\n",
    "    index_name='vectordb',\n",
    "    es_connection=es_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

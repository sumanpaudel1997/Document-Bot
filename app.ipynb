{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sumanpaudel/suman/Document-Bot/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.llms.huggingface_hub import HuggingFaceHub\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"Use the following pieces of context to answer the users question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "The \"SOURCES\" part should be a reference to the source of the document from which you got your answer.\n",
    "The answer must be lengthy. \n",
    "\n",
    "Example of your response should be:\n",
    "\n",
    "```\n",
    "The answer is foo\n",
    "SOURCES: xyz\n",
    "```\n",
    "\n",
    "Begin!\n",
    "----------------\n",
    "{summaries}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     SystemMessagePromptTemplate.from_template(system_template),\n",
    "#     HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "# ]\n",
    "# prompt = ChatPromptTemplate.from_messages(messages)\n",
    "# chain_type_kwargs = {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "loader = PyPDFLoader('spark.pdf')\n",
    "documents = loader.load()\n",
    "# Split the text into chunks\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs = {'device': 'cpu'},\n",
    "        encode_kwargs = {'normalize_embeddings': False})\n",
    "    \n",
    "docsearch = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "        llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=docsearch.as_retriever(),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Ballad of LangChain**\n",
      "\n",
      "In realms where code entwines,\n",
      "A language emerges, a beacon that shines.\n",
      "LangChain, the mighty, a tool so grand,\n",
      "Unveiling secrets in AI's vast land.\n",
      "\n",
      "With stanzas of data, it weaves its tale,\n",
      "A symphony of knowledge, without a fail.\n",
      "Tokens and models, a dance so divine,\n",
      "Generating insights, a vision so fine.\n",
      "\n",
      "Chorus:\n",
      "LangChain, LangChain, a marvel of might,\n",
      "Guiding our steps in AI's starry night.\n",
      "Its power boundless, like an endless sea,\n",
      "Unveiling truths, setting our minds free.\n",
      "\n",
      "From text to speech, a voice it can lend,\n",
      "Translating languages, breaking down the trend.\n",
      "Sentiment analysis, a keen eye it has,\n",
      "Unveiling emotions, shattering the glass.\n",
      "\n",
      "In games of strategy, it plans with grace,\n",
      "Predicting moves, winning with speed and pace.\n",
      "Medical marvels, it aids in their quest,\n",
      "Diagnosing ailments, giving us our best.\n",
      "\n",
      "Chorus:\n",
      "LangChain, LangChain, a marvel of might,\n",
      "Guiding our steps in AI's starry night.\n",
      "Its power boundless, like an endless sea,\n",
      "Unveiling truths, setting our minds free.\n",
      "\n",
      "But with great power comes a sacred trust,\n",
      "To wield it wisely, for purposes just.\n",
      "Let LangChain be a beacon of light,\n",
      "Guiding us towards a future bright.\n",
      "\n",
      "So let us sing the ballad of LangChain's might,\n",
      "A language that empowers, day and night.\n",
      "May its wisdom forever guide our way,\n",
      "In AI's realm, where we forever stay.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "result = llm.invoke(\"Write a ballad about LangChain\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

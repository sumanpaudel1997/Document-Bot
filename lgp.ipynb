{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sumanpaudel/suman/Document-Bot/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import google.generativeai as genai\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "loader = PyPDFLoader('spark.pdf')\n",
    "documents = loader.load()\n",
    "# Split the text into chunks\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#     model_kwargs = {'device': 'cpu'},\n",
    "#     encode_kwargs = {'normalize_embeddings': False})\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Answer the question as detailed as possible from the provided context, make sure to provide all the details,\n",
    "if the answer is not in the provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "Context:\\n {context}?\\n\n",
    "Question: \\n{question}\\n\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Initialize a ChatGoogleGenerativeAI model for conversational AI\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
    "\n",
    "# Create a prompt template with input variables \"context\" and \"question\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Load a question-answering chain with the specified model and prompt\n",
    "chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sumanpaudel/suman/Document-Bot/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_text': 'answer is not available in the context'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load a FAISS vector database from a local file\n",
    "new_db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Perform similarity search in the vector database based on the user question\n",
    "docs = new_db.similarity_search(\"tell me actions in spark with examples\")\n",
    "\n",
    "# Obtain a conversational question-answering chain\n",
    "\n",
    "\n",
    "# Use the conversational chain to get a response based on the user question and retrieved documents\n",
    "response = chain(\n",
    "    {\"input_documents\": docs, \"question\": \"tell me actions in spark with examples\"}, return_only_outputs=True\n",
    ")\n",
    "\n",
    "# Print the response to the console\n",
    "print(response)\n",
    "\n",
    "# Display the response in a Streamlit app (assuming 'st' is a Streamlit module)\n",
    "# st.write(\"Reply: \", response[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
